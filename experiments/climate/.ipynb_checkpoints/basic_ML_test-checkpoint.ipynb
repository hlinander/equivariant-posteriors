{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54a5543-bd37-4be8-8f04-4b45f7e04ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires to install eofs and gpytorch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gpytorch\n",
    "import os\n",
    "import glob\n",
    "from eofs.xarray import Eof\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict, Optional, List, Callable, Tuple, Union\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b6768-fb9b-450e-a101-51ec58eda85e",
   "metadata": {},
   "source": [
    "## GP functions modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e5a1a18-a103-4bae-8513-d8e899ae960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(mode: str = 'train'):\n",
    "    X, (so2_solver, bc_solver) = get_input_data(input_dir, mode)\n",
    "    y = get_output_data(target_dir, mode)\n",
    "    return torch.tensor(X), torch.tensor(y), (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def load_test_data(mode: str = 'train', solvers = None):\n",
    "    X, (so2_solver, bc_solver) = get_input_data(input_dir, mode, solvers)\n",
    "    y = get_output_data(target_dir, mode)\n",
    "    return torch.tensor(X), torch.tensor(y), (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def load_data_npz(path: str): #If np data already exists\n",
    "    X_train, y_train = np.load(os.path.join(base_dir, ''))\n",
    "    X_test, y_test = np.load(os.path.join(base_dir, ''))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def get_input_data(path: str, mode: str, solvers = None, n_eofs : int = 5):\n",
    "    train_experiments = [\"ssp126\", \"ssp370\"]  \n",
    "    # TODO: 585 har annat format\n",
    "    test_experiments = [\"ssp245\"]\n",
    "    input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']\n",
    "    fire_type = 'all-fires'\n",
    "\n",
    "    \n",
    "    BC = []\n",
    "    CH4 = []\n",
    "    CO2 = []\n",
    "    SO2 = []\n",
    "    \n",
    "    if mode == 'train':      \n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for exp in experiments:\n",
    "        print(exp)\n",
    "        for gas in input_gases:\n",
    "            input_dir = os.path.join(datapath, \"inputs\", \"input4mips\")\n",
    "            var_dir = os.path.join(input_dir, exp, gas, '250_km', 'mon')\n",
    "            files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "            #print(\"var dir\", var_dir)\n",
    "            #print(\"files\", files)\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    BC.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CH4_sum' and fire_type in f:\n",
    "                    CH4.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    SO2.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CO2_sum':\n",
    "                    CO2.append(f)\n",
    "    #print(\"BC\", BC)\n",
    "    print(\"opening datsets from paths\")\n",
    "    BC_data = xr.open_mfdataset(BC, concat_dim='time', combine='nested').compute().to_array()  # .to_numpy()\n",
    "    SO2_data = xr.open_mfdataset(SO2, concat_dim='time', combine='nested').compute() .to_array()  #.to_numpy()\n",
    "    CH4_data = xr.open_mfdataset(CH4, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    CO2_data = xr.open_mfdataset(CO2, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    \n",
    "    # BC_data = np.moveaxis(BC_data, 0, 1)\n",
    "    # SO2_data = np.moveaxis(SO2_data, 0, 1)\n",
    "    print(\"configuring data\")\n",
    "    CH4_data = np.moveaxis(CH4_data, 0, 1)\n",
    "    CO2_data = np.moveaxis(CO2_data, 0, 1)\n",
    "    CH4_data = CH4_data.reshape(CH4_data.shape[0], -1)\n",
    "    CO2_data = CO2_data.reshape(CO2_data.shape[0], -1)\n",
    "\n",
    "    \n",
    "    BC_data = BC_data.transpose('time', 'variable', 'lat', 'lon')\n",
    "    SO2_data = SO2_data.transpose('time', 'variable', 'lat', 'lon')\n",
    "    BC_data = BC_data.assign_coords(time=np.arange(len(BC_data.time)))\n",
    "    SO2_data = SO2_data.assign_coords(time=np.arange(len(SO2_data.time)))\n",
    "\n",
    "    \n",
    "    # Compute EOFs for BC\n",
    "    print(\"Solvers...\")\n",
    "    if solvers is None:\n",
    "        # print(BC_data.shape)\n",
    "        bc_solver = Eof(BC_data)\n",
    "        bc_eofs = bc_solver.eofsAsCorrelation(neofs=n_eofs)\n",
    "        bc_pcs = bc_solver.pcs(npcs=n_eofs, pcscaling=1)\n",
    "\n",
    "        # Compute EOFs for SO2\n",
    "        so2_solver = Eof(SO2_data)\n",
    "        so2_eofs = so2_solver.eofsAsCorrelation(neofs=n_eofs)\n",
    "        so2_pcs = so2_solver.pcs(npcs=n_eofs, pcscaling=1)\n",
    "\n",
    "        print(bc_pcs)\n",
    "\n",
    "        # Convert to pandas\n",
    "        bc_df = bc_pcs.to_dataframe().unstack('mode')\n",
    "        bc_df.columns = [f\"BC_{i}\" for i in range(n_eofs)]\n",
    "\n",
    "        so2_df = so2_pcs.to_dataframe().unstack('mode')\n",
    "        so2_df.columns = [f\"SO2_{i}\" for i in range(n_eofs)]\n",
    "    else:\n",
    "        so2_solver = solvers[0]\n",
    "        bc_solver = solvers[1]\n",
    "        \n",
    "        so2_pcs = so2_solver.projectField(SO2_data, neofs=n_eofs, eofscaling=1)\n",
    "        so2_df = so2_pcs.to_dataframe().unstack('mode')\n",
    "        so2_df.columns = [f\"SO2_{i}\" for i in range(n_eofs)]\n",
    "\n",
    "        bc_pcs = bc_solver.projectField(BC_data, neofs=n_eofs, eofscaling=1)\n",
    "        bc_df = bc_pcs.to_dataframe().unstack('mode')\n",
    "        bc_df.columns = [f\"BC_{i}\" for i in range(n_eofs)]\n",
    "    \n",
    "    CH4_data = CH4_data[:, :1]\n",
    "    CO2_data = CO2_data[:, :1]\n",
    "\n",
    "    print(bc_df.shape)\n",
    "    print(CH4_data.shape)\n",
    "    print(CO2_data.shape)\n",
    "    print(so2_df.shape)\n",
    "    print(\"merging data...\")\n",
    "    merged_data = np.concatenate((bc_df, CH4_data, CO2_data, so2_df), axis=1)\n",
    "    return merged_data, (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def get_output_data(path: str, mode: str):\n",
    "    total_ensembles = 1\n",
    "    nc_files = []\n",
    "    \n",
    "    if mode == 'train':\n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for mod in models:\n",
    "\n",
    "        model_dir = os.path.join(path, mod)\n",
    "        print(model_dir)\n",
    "        ensembles = os.listdir(model_dir)\n",
    "\n",
    "        if total_ensembles == 1:\n",
    "            ensembles = ensembles[0]\n",
    "        \n",
    "        exp_counter = 0\n",
    "        for exp in experiments:\n",
    "            for var in variables:\n",
    "                var_dir = os.path.join(path, mod, ensembles, exp, var, '250_km/mon')\n",
    "                files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "                nc_files += files\n",
    "        \n",
    "            if exp_counter == 0:\n",
    "                dataset = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "        \n",
    "            else: #concatenate dataset in time dimension\n",
    "                other_experiment = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "                dataset = np.concatenate((dataset, other_experiment), axis=1)\n",
    "                \n",
    "                \n",
    "            exp_counter += 1\n",
    "            \n",
    "        dataset = np.moveaxis(dataset, 0, 1)\n",
    "        print(dataset.shape)\n",
    "        dataset = dataset.reshape(dataset.shape[0], -1)\n",
    "        \n",
    "        # TODO: remove next line, only used for making quick tests\n",
    "        # print(\"dataset before\", dataset)\n",
    "        # dataset = dataset[:, :1]\n",
    "        # print(\"after\", dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd4d40-33e7-4813-9209-19e987ac4c59",
   "metadata": {},
   "source": [
    "## RF functions modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96a02d59-7312-4129-bf85-67f9b79bad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_rf(mode: str = 'train') -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = get_input_data_rf(input_dir, mode)\n",
    "    y = get_output_data_rf(target_dir, mode)\n",
    "    return X, y\n",
    "\n",
    "def load_data_npz(path: str): #If np data already exists\n",
    "    X_train, y_train = np.load(os.path.join(base_dir, ''))\n",
    "    X_test, y_test = np.load(os.path.join(base_dir, ''))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def get_input_data_rf(input_dir, mode: str):\n",
    "    BC = []\n",
    "    CH4 = []\n",
    "    CO2 = []\n",
    "    SO2 = []\n",
    "    train_experiments = [\"ssp126\", \"ssp370\"]  \n",
    "    test_experiments = [\"ssp245\"]\n",
    "    input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']\n",
    "    fire_type = 'all-fires'\n",
    "\n",
    "    if mode == 'train':      \n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for exp in experiments:\n",
    "        for gas in input_gases:\n",
    "            var_dir = os.path.join(input_dir, exp, gas, '250_km', 'mon')\n",
    "            files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    BC.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CH4_sum' and fire_type in f:\n",
    "                    CH4.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    SO2.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CO2_sum':\n",
    "                    CO2.append(f)\n",
    "\n",
    "    BC_data = xr.open_mfdataset(BC, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    CH4_data = xr.open_mfdataset(CH4, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    CO2_data = xr.open_mfdataset(CO2, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    SO2_data = xr.open_mfdataset(SO2, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "\n",
    "    merged_data = np.concatenate((BC_data, CH4_data, CO2_data, SO2_data), axis=0)\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def get_output_data_rf(output_dir: str, mode: str):\n",
    "    nc_files = []\n",
    "    \n",
    "    if mode == 'train':\n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for mod in models:\n",
    "\n",
    "        model_dir = os.path.join(output_dir, mod)\n",
    "        ensembles = os.listdir(model_dir)\n",
    "        print(model_dir)\n",
    "        print(ensembles)\n",
    "\n",
    "        total_ensembles = 1\n",
    "        if total_ensembles == 1:\n",
    "            ensembles = ensembles[0]\n",
    "        \n",
    "        exp_counter = 0\n",
    "        for exp in experiments:\n",
    "            for var in variables:\n",
    "                var_dir = os.path.join(output_dir, mod, ensembles, exp, var, '250_km/mon')\n",
    "                files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "                nc_files += files\n",
    "        \n",
    "            if exp_counter == 0:\n",
    "                dataset = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "        \n",
    "            else: #concatenate dataset in time dimension\n",
    "                other_experiment = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "                dataset = np.concatenate((dataset, other_experiment), axis=1)\n",
    "  \n",
    "            exp_counter += 1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82586cd5-55d0-4a84-b94f-0f3b6e3b3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datapath = \"/mnt/c/Users/Tage/00_Programming/5_Masters_thesis/Climateset_test/Dataset\"\n",
    "input_dir = os.path.join(datapath, \"inputs\", \"input4mips\")\n",
    "target_dir = os.path.join(datapath, \"outputs\", \"CMIP6\")\n",
    "\n",
    "fire_type = 'all-fires'\n",
    "variables = ['pr']\n",
    "models = ['CAS-ESM2-0']\n",
    "train_experiments = [\"ssp126\", \"ssp370\"] \n",
    "test_experiments = [\"ssp245\"]\n",
    "input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2933ce-d7ee-4d54-a29c-8949253de2fa",
   "metadata": {},
   "source": [
    "## RF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d342e94c-422f-4553-9051-58550af7dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Tage/00_Programming/5_Masters_thesis/Climateset_test/Dataset/outputs/CMIP6/CAS-ESM2-0\n",
      "['r3i1p1f1']\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data_rf('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9308b973-aff3-4b77-834c-314bf8f9cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Tage/00_Programming/5_Masters_thesis/Climateset_test/Dataset/outputs/CMIP6/CAS-ESM2-0\n",
      "['r3i1p1f1']\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data_rf('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45925560-a9ef-4625-b615-42ebb32e94ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2064, 96, 144) (1, 2064, 96, 144) (4, 1032, 96, 144) (1, 1032, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b29b662-8d8f-49ea-a342-0e496e2a878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2064, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "X_stat = X_train.copy()\n",
    "print(X_stat.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3308579-d4fa-4f2f-8931-441e9df202c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (4,)\n",
      "[3.1539596e-13 1.7624520e-11 1.8632094e-09 3.1539596e-13] [1.98722549e-12 1.00920716e-10 1.86915941e-08 1.98722549e-12]\n",
      "(4, 1, 1, 1)\n",
      "(4, 1, 1, 1)\n",
      "(4, 2064, 96, 144)\n",
      "[2.711436e-05] [2.711436e-05]\n",
      "(4, 258, 12, 96, 144)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vars_mean = np.mean(X_stat, axis=(1, 2, 3))\n",
    "vars_std = np.std(X_stat, axis=(1, 2, 3))\n",
    "\n",
    "print(vars_mean.shape, vars_std.shape)\n",
    "\n",
    "print(vars_mean, vars_std)\n",
    "\n",
    "input_stats = np.concatenate((np.expand_dims(vars_mean, (-1, 1, 2, 3)), np.expand_dims(vars_std, (-1, 1, 2, 3))), axis=-1)\n",
    "\n",
    "input_stats\n",
    "\n",
    "x = np.array([3.1055716e-13, 1.9952876e-11, 2.7081224e-09, 3.1055716e-13])\n",
    "\n",
    "x = np.expand_dims(x, (1, 2, 3))\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "vars_mean = np.expand_dims(vars_mean, (1, 2, 3))\n",
    "vars_std = np.expand_dims(vars_std, (1, 2, 3))\n",
    "print(vars_mean.shape)\n",
    "X_norm = (X_stat - vars_mean)/ vars_std\n",
    "\n",
    "print(X_norm.shape)\n",
    "\n",
    "y_train.shape\n",
    "\n",
    "out_mean = np.mean(y_train, axis=(1, 2, 3))\n",
    "out_std = np.mean(y_train, axis=(1, 2, 3))\n",
    "\n",
    "print(out_mean, out_std)\n",
    "\n",
    "y_norm = (y_train - out_mean)/(out_std)\n",
    "\n",
    "# (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "z = np.zeros((258, 12, 4, 96, 144))\n",
    "\n",
    "z = np.moveaxis(z, 2, 0)\n",
    "print(z.shape)\n",
    "\n",
    "z_max = np.min(z, (1, 2, 3, 4))\n",
    "\n",
    "print(z_max.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cedb630f-12f9-4ce3-bb57-14d829e1e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**parameters & hyperparameters\n",
    "\n",
    "RSCV= True\n",
    "path_output='output_path/output.nc'\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5,55, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [5, 10, 15, 25]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [4, 8, 12]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02a07b14-84cd-43d0-b2b7-e44501e6bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5df7b8b-881e-45e4-a8fa-d21ce7bf2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg0 = RandomForestRegressor(random_state=0)\n",
    "rf_random0 = RandomizedSearchCV(estimator = reg0, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1d9b93c-f1b1-472b-a301-7ca5c3dfffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 4, 96, 144) (1032, 1, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.moveaxis(X_test, 0, 1)\n",
    "y_test = np.moveaxis(y_test, 0, 1)\n",
    "\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "834a9d4a-e6d6-4b13-a1d4-1b3c8636af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "y_test = y_test.reshape(y_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0c62c33-55cc-4a11-8302-b4ca9c6e8431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 55296) (1032, 13824)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c47cd8-8fcf-4aa7-8306-a02d54bda963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "rf_pr = rf_random0.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ccff1d-47a4-4ac8-a510-a39b4240ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_pr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c531f-1810-47da-beab-3cc45d4efefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.moveaxis(X_train, 0, 1)\n",
    "y_train = np.moveaxis(y_train, 0, 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "y_train = y_train.reshape(y_train.shape[0], -1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf1151-fa8f-48fc-bb8e-2dbe2a7f6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_pr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d67593-d6d4-4915-a0dc-2b98adba25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964275e-826f-4869-9b02-47b7b9399959",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(3096, 1, 96, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c149cd5-3430-46a1-9e7b-fb4785ea8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_train, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea6107-3b86-47bd-81ed-896f47796fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c1bf7-6e1e-4715-a554-a81e04f56bd5",
   "metadata": {},
   "source": [
    "## GP test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b828875-15e5-4bec-9dc9-64470ca31fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssp126\n",
      "ssp370\n",
      "opening datsets from paths\n",
      "configuring data\n",
      "Solvers...\n",
      "<xarray.DataArray 'pcs' (time: 2064, mode: 5)> Size: 41kB\n",
      "array([[ 1.0219064 ,  1.1170815 ,  0.72184145,  2.584413  , -0.8731214 ],\n",
      "       [ 0.8631581 ,  0.8479874 ,  0.37466732,  1.6350561 ,  0.613982  ],\n",
      "       [ 0.69782346,  0.79465306,  0.33857268,  0.88862455,  1.3863869 ],\n",
      "       ...,\n",
      "       [ 0.59065527, -0.77790904,  0.997609  , -2.447385  ,  0.18884279],\n",
      "       [ 0.5087815 ,  0.5143082 ,  0.48934758, -2.3020062 , -0.61008877],\n",
      "       [ 0.52770656,  0.70695853,  0.53608197, -1.5437057 , -1.5951791 ]],\n",
      "      shape=(2064, 5), dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) int64 17kB 0 1 2 3 4 5 6 ... 2058 2059 2060 2061 2062 2063\n",
      "  * mode     (mode) int64 40B 0 1 2 3 4\n",
      "(2064, 5)\n",
      "(2064, 1)\n",
      "(2064, 1)\n",
      "(2064, 5)\n",
      "merging data...\n",
      "/mnt/c/Users/Tage/00_Programming/5_Masters_thesis/Climateset_test/Dataset/outputs/CMIP6/CAS-ESM2-0\n",
      "(3096, 1, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, (so2_solver, bc_solver) = load_train_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc9b7f5-acd8-4f27-881d-788f15ab57bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssp245\n",
      "opening datsets from paths\n",
      "configuring data\n",
      "Solvers...\n",
      "(1032, 5)\n",
      "(1032, 1)\n",
      "(1032, 1)\n",
      "(1032, 5)\n",
      "merging data...\n",
      "/mnt/c/Users/Tage/00_Programming/5_Masters_thesis/Climateset_test/Dataset/outputs/CMIP6/CAS-ESM2-0\n",
      "(1032, 1, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, (so2_solver, bc_solver) = load_test_data('test', (so2_solver, bc_solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd125e66-45c0-49ab-843a-da7baf33b336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2064, 12]), torch.Size([3096, 13824]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfb1beeb-06ec-48d1-816d-0b55fe81f891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1032, 12]), torch.Size([1032, 13824]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b7b2da2-4837-4835-9637-aa995f5f083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for test:\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # global\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "training_data = ClimateDataset(X_train, y_train)\n",
    "test_data = ClimateDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "030e3432-4abc-4b17-99e2-b0e9b559b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_inducing_points = 500\n",
    "n_epochs = 2  # Could use criterion to stop\n",
    "lr = 0.1\n",
    "# optimizer = adam\n",
    "# kernel = matern3/2\n",
    "\n",
    "class ApproxGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, num_tasks):\n",
    "        # inducing_points size: num_outputs, num_examples, num_features\n",
    "        inducing_points = inducing_points.reshape(1, inducing_points.size(0), -1)\n",
    "        # inducing_points = inducing_points.repeat(num_tasks, 1, 1)\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2), batch_shape=torch.Size([num_tasks]))\n",
    "\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=num_tasks\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_tasks]))\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, batch_shape=torch.Size([num_tasks])), batch_shape=torch.Size([num_tasks]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class GaussianProcess(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inducing_points,\n",
    "                 num_out_var):\n",
    "        super().__init__()\n",
    "        self.model = ApproxGPModel(inducing_points=inducing_points, num_tasks=num_out_var)\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_out_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = self.likelihood(self.model(X))\n",
    "        return predictions.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea286d4e-4360-4bd8-a708-f28deb3e1b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 12])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.X[:num_inducing_points].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8797de6f-c6de-4b66-be4c-7df784d84192",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 13824000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gp_model = \u001b[43mGaussianProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_inducing_points\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mGaussianProcess.__init__\u001b[39m\u001b[34m(self, inducing_points, num_out_var)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m     35\u001b[39m              inducing_points,\n\u001b[32m     36\u001b[39m              num_out_var):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mApproxGPModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minducing_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43minducing_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tasks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_out_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_out_var)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mApproxGPModel.__init__\u001b[39m\u001b[34m(self, inducing_points, num_tasks)\u001b[39m\n\u001b[32m     11\u001b[39m inducing_points = inducing_points.reshape(\u001b[32m1\u001b[39m, inducing_points.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# inducing_points = inducing_points.repeat(num_tasks, 1, 1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m variational_distribution = \u001b[43mgpytorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvariational\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCholeskyVariationalDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43minducing_points\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_tasks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n\u001b[32m     16\u001b[39m     gpytorch.variational.VariationalStrategy(\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mself\u001b[39m, inducing_points, variational_distribution, learn_inducing_locations=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m     ),\n\u001b[32m     19\u001b[39m     num_tasks=num_tasks\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(variational_strategy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/climate/lib/python3.11/site-packages/gpytorch/variational/cholesky_variational_distribution.py:37\u001b[39m, in \u001b[36mCholeskyVariationalDistribution.__init__\u001b[39m\u001b[34m(self, num_inducing_points, batch_shape, mean_init_std, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m covar_init = torch.eye(num_inducing_points, num_inducing_points)\n\u001b[32m     36\u001b[39m mean_init = mean_init.repeat(*batch_shape, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m covar_init = \u001b[43mcovar_init\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mself\u001b[39m.register_parameter(name=\u001b[33m\"\u001b[39m\u001b[33mvariational_mean\u001b[39m\u001b[33m\"\u001b[39m, parameter=torch.nn.Parameter(mean_init))\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m.register_parameter(name=\u001b[33m\"\u001b[39m\u001b[33mchol_variational_covar\u001b[39m\u001b[33m\"\u001b[39m, parameter=torch.nn.Parameter(covar_init))\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 13824000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "gp_model = GaussianProcess(training_data.X[:num_inducing_points], training_data.y.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c52ed-7dc3-4ca1-9143-ad43bfce5b9c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44ff01ea-1f70-47b9-af12-5e291423d0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcess(\n",
       "  (model): ApproxGPModel(\n",
       "    (variational_strategy): IndependentMultitaskVariationalStrategy(\n",
       "      (base_variational_strategy): VariationalStrategy(\n",
       "        (_variational_distribution): CholeskyVariationalDistribution()\n",
       "      )\n",
       "    )\n",
       "    (mean_module): ConstantMean()\n",
       "    (covar_module): ScaleKernel(\n",
       "      (base_kernel): MaternKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (likelihood): MultitaskGaussianLikelihood(\n",
       "    (raw_task_noises_constraint): GreaterThan(1.000E-04)\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': gp_model.parameters()}\n",
    "], lr=lr)\n",
    "mll = gpytorch.mlls.VariationalELBO(gp_model.likelihood, gp_model.model, num_data=training_data.y.size(0))\n",
    "gp_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfa8bcee-8138-40d8-ae4d-99edba91892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "for i in range(n_epochs):\n",
    "    print(f\"epoch #{i}\")\n",
    "    for x, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = gp_model(x)\n",
    "        loss = -mll(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec15dca5-0f50-4351-bb27-a6d8a782193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "gp_model.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = gp_model.likelihood(gp_model(test_data.X))\n",
    "    y_pred = predictions.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c43f43d-232d-41f3-9570-53fc5c48d406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.157426575256977e-06"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = mean_squared_error(test_data.y, y_pred)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2e24d-d422-47ce-bcf5-0c135640fdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
