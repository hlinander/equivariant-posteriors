{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7d05054-416e-48d8-97c1-29171cc1e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: healpix in /home/tage/miniconda3/envs/climate/lib/python3.11/site-packages (2025.1)\n",
      "Requirement already satisfied: numpy in /home/tage/miniconda3/envs/climate/lib/python3.11/site-packages (from healpix) (2.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install healpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbad7f41-00c6-4c04-970a-1027e0ad6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires to install eofs and gpytorch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import gpytorch\n",
    "import os\n",
    "import glob\n",
    "from eofs.xarray import Eof\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict, Optional, List, Callable, Tuple, Union\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f83f347b-1b6c-4f57-b10c-6ebeb6738481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(mode: str = 'train'):\n",
    "    X, (so2_solver, bc_solver) = get_input_data(input_dir, mode)\n",
    "    y = get_output_data(target_dir, mode)\n",
    "    return torch.tensor(X), torch.tensor(y), (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def load_test_data(mode: str = 'train', solvers = None):\n",
    "    X, (so2_solver, bc_solver) = get_input_data(input_dir, mode, solvers)\n",
    "    y = get_output_data(target_dir, mode)\n",
    "    return torch.tensor(X), torch.tensor(y), (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def load_data_npz(path: str): #If np data already exists\n",
    "    X_train, y_train = np.load(os.path.join(base_dir, ''))\n",
    "    X_test, y_test = np.load(os.path.join(base_dir, ''))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def get_input_data(path: str, mode: str, solvers = None, n_eofs : int = 5):\n",
    "    # train_experiments = [\"ssp126\", \"ssp370\"]\n",
    "    train_experiments = [\"ssp126\"]\n",
    "    # TODO: 585 har annat format\n",
    "    test_experiments = [\"ssp245\"]\n",
    "    input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']\n",
    "    fire_type = 'all-fires'\n",
    "\n",
    "    \n",
    "    BC = []\n",
    "    CH4 = []\n",
    "    CO2 = []\n",
    "    SO2 = []\n",
    "    \n",
    "    if mode == 'train':      \n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for exp in experiments:\n",
    "        print(exp)\n",
    "        for gas in input_gases:\n",
    "            input_dir = os.path.join(datapath, \"inputs\", \"input4mips\")\n",
    "            var_dir = os.path.join(input_dir, exp, gas, '250_km', 'mon')\n",
    "            files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "            #print(\"var dir\", var_dir)\n",
    "            #print(\"files\", files)\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    BC.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CH4_sum' and fire_type in f:\n",
    "                    CH4.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'BC_sum' and fire_type in f:\n",
    "                    SO2.append(f)\n",
    "            for f in files:\n",
    "                if gas == 'CO2_sum':\n",
    "                    CO2.append(f)\n",
    "    #print(\"BC\", BC)\n",
    "    print(\"opening datsets from paths\")\n",
    "    BC_data = xr.open_mfdataset(BC, concat_dim='time', combine='nested').compute().to_array()  # .to_numpy()\n",
    "    SO2_data = xr.open_mfdataset(SO2, concat_dim='time', combine='nested').compute() .to_array()  #.to_numpy()\n",
    "    CH4_data = xr.open_mfdataset(CH4, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    CO2_data = xr.open_mfdataset(CO2, concat_dim='time', combine='nested').compute().to_array().to_numpy()\n",
    "    print(BC_data.shape)\n",
    "    # BC_data = np.moveaxis(BC_data, 0, 1)\n",
    "    # SO2_data = np.moveaxis(SO2_data, 0, 1)\n",
    "    print(\"configuring data\")\n",
    "    CH4_data = np.moveaxis(CH4_data, 0, 1)\n",
    "    CO2_data = np.moveaxis(CO2_data, 0, 1)\n",
    "    CH4_data = CH4_data.reshape(CH4_data.shape[0], -1)\n",
    "    CO2_data = CO2_data.reshape(CO2_data.shape[0], -1)\n",
    "\n",
    "    return \n",
    "    BC_data = BC_data.transpose('time', 'variable', 'lat', 'lon')\n",
    "    SO2_data = SO2_data.transpose('time', 'variable', 'lat', 'lon')\n",
    "    BC_data = BC_data.assign_coords(time=np.arange(len(BC_data.time)))\n",
    "    SO2_data = SO2_data.assign_coords(time=np.arange(len(SO2_data.time)))\n",
    "\n",
    "    \n",
    "    # Compute EOFs for BC\n",
    "    print(\"Solvers...\")\n",
    "    if solvers is None:\n",
    "        # print(BC_data.shape)\n",
    "        bc_solver = Eof(BC_data)\n",
    "        bc_eofs = bc_solver.eofsAsCorrelation(neofs=n_eofs)\n",
    "        bc_pcs = bc_solver.pcs(npcs=n_eofs, pcscaling=1)\n",
    "\n",
    "        # Compute EOFs for SO2\n",
    "        so2_solver = Eof(SO2_data)\n",
    "        so2_eofs = so2_solver.eofsAsCorrelation(neofs=n_eofs)\n",
    "        so2_pcs = so2_solver.pcs(npcs=n_eofs, pcscaling=1)\n",
    "\n",
    "        print(bc_pcs)\n",
    "\n",
    "        # Convert to pandas\n",
    "        bc_df = bc_pcs.to_dataframe().unstack('mode')\n",
    "        bc_df.columns = [f\"BC_{i}\" for i in range(n_eofs)]\n",
    "\n",
    "        so2_df = so2_pcs.to_dataframe().unstack('mode')\n",
    "        so2_df.columns = [f\"SO2_{i}\" for i in range(n_eofs)]\n",
    "    else:\n",
    "        so2_solver = solvers[0]\n",
    "        bc_solver = solvers[1]\n",
    "        \n",
    "        so2_pcs = so2_solver.projectField(SO2_data, neofs=n_eofs, eofscaling=1)\n",
    "        so2_df = so2_pcs.to_dataframe().unstack('mode')\n",
    "        so2_df.columns = [f\"SO2_{i}\" for i in range(n_eofs)]\n",
    "\n",
    "        bc_pcs = bc_solver.projectField(BC_data, neofs=n_eofs, eofscaling=1)\n",
    "        bc_df = bc_pcs.to_dataframe().unstack('mode')\n",
    "        bc_df.columns = [f\"BC_{i}\" for i in range(n_eofs)]\n",
    "    \n",
    "    CH4_data = CH4_data[:, :1]\n",
    "    CO2_data = CO2_data[:, :1]\n",
    "\n",
    "    print(bc_df.shape)\n",
    "    print(CH4_data.shape)\n",
    "    print(CO2_data.shape)\n",
    "    print(so2_df.shape)\n",
    "    print(\"merging data...\")\n",
    "    merged_data = np.concatenate((bc_df, CH4_data, CO2_data, so2_df), axis=1)\n",
    "    return merged_data, (so2_solver, bc_solver)\n",
    "\n",
    "\n",
    "def get_output_data(path: str, mode: str):\n",
    "    total_ensembles = 1\n",
    "    nc_files = []\n",
    "    \n",
    "    if mode == 'train':\n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for mod in models:\n",
    "\n",
    "        model_dir = os.path.join(path, mod)\n",
    "        print(model_dir)\n",
    "        ensembles = os.listdir(model_dir)\n",
    "\n",
    "        if total_ensembles == 1:\n",
    "            ensembles = ensembles[0]\n",
    "        \n",
    "        exp_counter = 0\n",
    "        for exp in experiments:\n",
    "            for var in variables:\n",
    "                var_dir = os.path.join(path, mod, ensembles, exp, var, '250_km/mon')\n",
    "                files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "                nc_files += files\n",
    "        \n",
    "            if exp_counter == 0:\n",
    "                dataset = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "        \n",
    "            else: #concatenate dataset in time dimension\n",
    "                other_experiment = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "                dataset = np.concatenate((dataset, other_experiment), axis=1)\n",
    "                \n",
    "                \n",
    "            exp_counter += 1\n",
    "            \n",
    "        dataset = np.moveaxis(dataset, 0, 1)\n",
    "        print(dataset.shape)\n",
    "        dataset = dataset.reshape(dataset.shape[0], -1)\n",
    "        \n",
    "        # TODO: remove next line, only used for making quick tests\n",
    "        dataset = dataset[:, :1]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aea1d571-f23f-4907-a428-fb9dcff0959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datapath = \"/mnt/c/Users/Tage/00_Programming/5_Masters_thesis/Climateset_test/Dataset\"\n",
    "\n",
    "#current_dir = os.getcwd()\n",
    "input_dir = os.path.join(datapath, \"inputs\", \"input4mips\")\n",
    "target_dir = os.path.join(datapath, \"outputs\", \"CMIP6\")\n",
    "\n",
    " # MOVED INSIDE FUNCTION!!\n",
    "fire_type = 'all-fires'\n",
    "variables = ['pr']\n",
    "models = ['CAS-ESM2-0']\n",
    "#train_experiments = [\"ssp585\", \"ssp126\", \"ssp370\"] \n",
    "#test_experiments = [\"ssp245\"]\n",
    "# ------------------------\n",
    "\n",
    "input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2970298-7cf2-4b91-a353-5fed0d5aa434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssp126\n",
      "opening datsets from paths\n",
      "(1, 1032, 96, 144)\n",
      "configuring data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, y_train, (so2_solver, bc_solver) = \u001b[43mload_train_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mload_train_data\u001b[39m\u001b[34m(mode)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_train_data\u001b[39m(mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     X, (so2_solver, bc_solver) = get_input_data(input_dir, mode)\n\u001b[32m      3\u001b[39m     y = get_output_data(target_dir, mode)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(X), torch.tensor(y), (so2_solver, bc_solver)\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "X_train, y_train, (so2_solver, bc_solver) = load_train_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7551ed80-db1f-462f-a4c5-4f4be5cebdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40266fd-3e38-4191-a352-091125f067d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpix\n",
    "import healpy\n",
    "\n",
    "def interpolate_dh_to_hp(nside, variable: xr.DataArray):\n",
    "    npix = healpix.nside2npix(nside)\n",
    "    hlong, hlat = healpix.pix2ang(nside, np.arange(0, npix, 1), lonlat=True, nest=True)\n",
    "    hlong = np.mod(hlong, 360)\n",
    "    xlong = xr.DataArray(hlong, dims=\"z\")\n",
    "    xlat = xr.DataArray(hlat, dims=\"z\")\n",
    "\n",
    "    xhp = variable.interp(latitude=xlat, longitude=xlong, kwargs={\"fill_value\": None})\n",
    "    hp_image = np.array(xhp.to_array().to_numpy(), dtype=np.float32)\n",
    "    return hp_image\n",
    "\n",
    "\n",
    "def e5_to_numpy_hp(e5xr, nside: int, normalized: bool):\n",
    "\n",
    "    hp_surface = interpolate_dh_to_hp(nside, e5xr.surface)\n",
    "    hp_upper = interpolate_dh_to_hp(nside, e5xr.upper)\n",
    "\n",
    "    if normalized:\n",
    "        stats = deserialize_dataset_statistics(nside)\n",
    "        hp_surface, hp_upper = normalize_sample(stats.item(), hp_surface, hp_upper)\n",
    "\n",
    "    return hp_surface, hp_upper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
