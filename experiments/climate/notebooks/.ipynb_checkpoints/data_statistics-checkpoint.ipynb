{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a143e7-373b-4f40-89d2-07864feb84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[1]  # or resolve explicitly\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43be184-ff73-42e8-93ae-46bdaafd6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires to install eofs and gpytorch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import gpytorch\n",
    "import os\n",
    "import glob\n",
    "#from eofs.xarray import Eof\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict, Optional, List, Callable, Tuple, Union\n",
    "\n",
    "#import wandb\n",
    "#from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95619212-5139-4afc-a2ab-8cee9fc2fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_channel_statistics(experiment_dict, gas_vars, log_transform=False, plot=False):\n",
    "    \"\"\"\n",
    "    Compute mean and std for each channel separately.\n",
    "    Returns dict with stats for each variable.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for var in gas_vars:\n",
    "        all_values = []\n",
    "        for exp in experiment_dict.keys():\n",
    "            data = experiment_dict[exp][var]\n",
    "            # Log transform for gases (very small values)\n",
    "            \n",
    "            if var in ['BC', 'CH4', 'SO2', 'CO2'] and log_transform:\n",
    "                data_log = np.log10(np.clip(data, 1e-15, None) + 1e-15)\n",
    "                all_values.append(data_log.flatten())\n",
    "            else:\n",
    "                all_values.append(data.flatten()) # No log for temperature/other outputs\n",
    "        \n",
    "        all_values = np.concatenate(all_values)\n",
    "        stats[var] = {\n",
    "            'mean': np.mean(all_values),\n",
    "            'std': np.std(all_values),\n",
    "            'use_log': var in ['BC', 'CH4', 'SO2', 'CO2']\n",
    "        }\n",
    "        print(f\"{var}: mean={stats[var]['mean']:.6f}, std={stats[var]['std']:.6f}, log={stats[var]['use_log']}\")\n",
    "        if plot:\n",
    "            plt.title(f\"{exp}: {var}\")\n",
    "            plt.hist(all_values, bins=50)\n",
    "            plt.show()\n",
    "            if var in ['BC', 'CH4', 'SO2', 'CO2'] and not log_transform:\n",
    "                plt.title(f\"{exp}: {var} log-transformed\")\n",
    "                plt.hist(np.log10(np.clip(all_values, 1e-15, None) + 1e-15), bins=50)\n",
    "                plt.show()\n",
    "    \n",
    "            z_norm = (all_values - np.mean(all_values)) / np.std(all_values)\n",
    "            plt.title(f\"{exp}: {var} z-normed\")\n",
    "            plt.hist(z_norm, bins=50)\n",
    "            plt.show()\n",
    "\n",
    "    return stats\n",
    "\n",
    "def compute_per_channel_statistics_linear_z(\n",
    "    experiment_dict,\n",
    "    gas_vars,\n",
    "    plot=False,\n",
    "):\n",
    "    stats = {}\n",
    "\n",
    "    for var in gas_vars:\n",
    "        all_values = []\n",
    "\n",
    "        for exp in experiment_dict.keys():\n",
    "            data = experiment_dict[exp][var]\n",
    "            all_values.append(data.flatten())\n",
    "\n",
    "        all_values = np.concatenate(all_values)\n",
    "\n",
    "        mean = np.mean(all_values)\n",
    "        std = np.std(all_values)\n",
    "\n",
    "        stats[var] = {\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"use_log\": False\n",
    "        }\n",
    "\n",
    "        print(f\"{var}: mean={mean:.6e}, std={std:.6e}, log=False\")\n",
    "        if plot:\n",
    "            # Raw distribution\n",
    "            plt.figure()\n",
    "            plt.title(f\"{var} raw (linear)\")\n",
    "            plt.hist(all_values, bins=50)\n",
    "            plt.show()\n",
    "    \n",
    "            # Z-scored distribution\n",
    "            z = (all_values - mean) / std\n",
    "            plt.figure()\n",
    "            plt.title(f\"{var} z-score (linear)\")\n",
    "            plt.hist(z, bins=50)\n",
    "            plt.show()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56564d44-9427-44bc-98a5-20a2e50a74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpix\n",
    "def interpolate_dh_to_hp(nside, variable: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Input is xr.DataArray \n",
    "    \"\"\"\n",
    "    \n",
    "    npix = healpix.nside2npix(nside)\n",
    "    hlong, hlat = healpix.pix2ang(nside, np.arange(0, npix, 1), lonlat=True, nest=True)\n",
    "    hlong = np.mod(hlong, 360)\n",
    "    xlong = xr.DataArray(hlong, dims=\"z\")\n",
    "    xlat = xr.DataArray(hlat, dims=\"z\")\n",
    "\n",
    "    xhp = variable.interp(lat=xlat, lon=xlong, kwargs={\"fill_value\": None})\n",
    "    hp_image = np.array(xhp.to_numpy(), dtype=np.float32) # ! removed to_array()\n",
    "    return hp_image\n",
    "\n",
    "def interpolate_dh_to_hp_output(nside, variable: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Input is xr.DataArray \n",
    "    \"\"\"\n",
    "    \n",
    "    npix = healpix.nside2npix(nside)\n",
    "    hlong, hlat = healpix.pix2ang(nside, np.arange(0, npix, 1), lonlat=True, nest=True)\n",
    "    hlong = np.mod(hlong, 360)\n",
    "    xlong = xr.DataArray(hlong, dims=\"z\")\n",
    "    xlat = xr.DataArray(hlat, dims=\"z\")\n",
    "\n",
    "    xhp = variable.interp(y=xlat, x=xlong, kwargs={\"fill_value\": None})\n",
    "    hp_image = np.array(xhp.to_numpy(), dtype=np.float32) # ! removed to_array()\n",
    "    return hp_image\n",
    "\n",
    "def e5_to_numpy_hp(e5xr, nside: int, normalized: bool):\n",
    "    \"\"\"\n",
    "    Input is class with xr.DataArray class variables\n",
    "    \"\"\"\n",
    "\n",
    "    hp_surface = interpolate_dh_to_hp(nside, e5xr.surface)\n",
    "    hp_upper = interpolate_dh_to_hp(nside, e5xr.upper)\n",
    "\n",
    "    if normalized:\n",
    "        stats = deserialize_dataset_statistics(nside)\n",
    "        hp_surface, hp_upper = normalize_sample(stats.item(), hp_surface, hp_upper)\n",
    "\n",
    "    return hp_surface, hp_upper\n",
    "\n",
    "def get_input_paths(exp, input_dir, fire_type):\n",
    "    input_gasses = {\n",
    "    \"BC\":  \"BC_sum\",\n",
    "    \"CH4\": \"CH4_sum\",\n",
    "    \"SO2\": \"SO2_sum\",\n",
    "    \"CO2\": \"CO2_sum\",\n",
    "    }\n",
    "    gas_files = {g: [] for g in gas_patterns}\n",
    "    for gas, folder_name in input_gasses.items():\n",
    "        var_dir = os.path.join(input_dir, exp, folder_name, \"250_km\", \"mon\")\n",
    "        files = glob.glob(var_dir + \"/**/*.nc\", recursive=True)\n",
    "    \n",
    "        for f in files:\n",
    "            if folder_name in f and (gas == \"CO2\" or fire_type in f): # CO2 does not have fire_type\n",
    "                gas_files[gas].append(f)\n",
    "    #print(gas_files[\"BC\"])\n",
    "    # Check same len\n",
    "    for k, v in gas_files.items():\n",
    "        print(k, len(v))\n",
    "\n",
    "    return gas_files\n",
    "\n",
    "def get_output_paths(exp, target_dir, mod, ensembles, variables):\n",
    "    var_file_dict = {v: [] for v in variables}\n",
    "    for var in variables:\n",
    "        var_dir = os.path.join(target_dir, mod, ensembles, exp, var, '250_km', 'mon')\n",
    "        var_files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "        #var_file_dict[var].append(var_files)\n",
    "        for f in var_files:\n",
    "            var_file_dict[var].append(f)\n",
    "\n",
    "    return var_file_dict\n",
    "\n",
    "def get_hp_dataset(files : dict, nside : int, output : bool = False):\n",
    "    hp_gas_dict = {}\n",
    "    #print(files)\n",
    "    for var, var_files in files.items():\n",
    "\n",
    "        \n",
    "        # ds = xr.open_mfdataset(var_files, concat_dim=\"time\", combine=\"nested\")\n",
    "        # this required dask, which is not currently in constructed apptainer\n",
    "\n",
    "        datasets = [xr.open_dataset(f) for f in var_files]\n",
    "        ds = xr.concat(datasets, dim=\"time\").sortby(\"time\")\n",
    "        \n",
    "        ds = ds.sortby(\"time\")\n",
    "        arr = ds.to_array().squeeze(\"variable\") # remove var dim since 1\n",
    "\n",
    "        if output:\n",
    "            hp = interpolate_dh_to_hp_output(nside, arr)\n",
    "        else:\n",
    "            hp = interpolate_dh_to_hp(nside, arr)\n",
    "        # print(type(hp))\n",
    "        hp_gas_dict[var] = hp\n",
    "\n",
    "    return hp_gas_dict\n",
    "\n",
    "import healpy\n",
    "def healpix_plotting(hp_data, data_shift=0, norm=None, nest=True):\n",
    "    data = hp_data + data_shift\n",
    "    healpy.visufunc.cartview(data, nest=nest, norm=norm)\n",
    "    #healpy.visufunc.orthview(data, nest=nest, norm=norm)\n",
    "    #healpy.visufunc.mollview(data, nest=nest, norm=norm)\n",
    "    #healpy.visufunc.gnomview(data, nest=nest, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c04f9-ff81-438f-9428-335d50798727",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"/proj/heal_pangu/users/x_tagty/climateset\"\n",
    "input_dir = os.path.join(datapath, \"inputs\", \"input4mips\")\n",
    "target_dir = os.path.join(datapath, \"outputs\", \"CMIP6\")\n",
    "\n",
    "fire_type = 'all-fires'\n",
    "output_vars = ['tas', \"pr\"]\n",
    "\n",
    "mod = 'CAS-ESM2-0'\n",
    "ensembles = 'r3i1p1f1'\n",
    "experiments = [\"ssp585\", \"ssp126\", \"ssp370\"]\n",
    "#experiments = [train_experiments[0]]\n",
    "#input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']\n",
    "gas_patterns = {\n",
    "    \"BC\":  \"BC_sum\",\n",
    "    \"CH4\": \"CH4_sum\",\n",
    "    \"SO2\": \"SO2_sum\",\n",
    "    \"CO2\": \"CO2_sum\",\n",
    "}\n",
    "\n",
    "nside = 32\n",
    "experiment_input_dict = {}\n",
    "experiment_output_dict = {}\n",
    "for exp in experiments:\n",
    "    input_paths = get_input_paths(exp, input_dir, fire_type)\n",
    "    print(\"got input paths\")\n",
    "    hp_input_dict = get_hp_dataset(input_paths, nside)\n",
    "    print(\"hp input done\")\n",
    "    # TODO : Add concatenate or similar if multiple experiments\n",
    "    experiment_input_dict[exp] = hp_input_dict\n",
    "    \n",
    "    print(\"checking outputs\")\n",
    "    output_paths = get_output_paths(exp, target_dir, mod, ensembles, output_vars)\n",
    "    print(\"got output paths\")\n",
    "    hp_target_dict = get_hp_dataset(output_paths, nside, output=True)\n",
    "    # TODO : Add concatenate or similar  if multiple experiments\n",
    "    experiment_output_dict[exp] = hp_target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00deae-64a2-4627-be29-5bc875db9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_manual_batches(experiment_input_dict, experiment_output_dict, \n",
    "                           gas_vars=['BC', 'CH4', 'SO2', 'CO2'],\n",
    "                           output_vars=[\"tas\", \"pr\"],\n",
    "                           batch_size=8):\n",
    "    \"\"\"\n",
    "    Manually create batches without using DataLoader.\n",
    "    Returns a list of (input_batch, output_batch) tuples.\n",
    "    \"\"\"\n",
    "    all_inputs = []\n",
    "    all_outputs = []\n",
    "    all_metadata = []\n",
    "    print(experiment_output_dict)\n",
    "    # Collect all samples\n",
    "    for exp in experiment_input_dict.keys():\n",
    "        n_timesteps = experiment_input_dict[exp][gas_vars[0]].shape[0]\n",
    "        \n",
    "        for t in range(n_timesteps):\n",
    "            # Stack input channels\n",
    "            input_channels = [experiment_input_dict[exp][var][t] for var in gas_vars]\n",
    "            input_data = np.stack(input_channels, axis=0)  # (C, N)\n",
    "            all_inputs.append(input_data)\n",
    "            \n",
    "            # Stack output channels (if doing supervised learning)\n",
    "            if experiment_output_dict and exp in experiment_output_dict:\n",
    "                output_channels = [experiment_output_dict[exp][var][t] for var in output_vars]\n",
    "                output_data = np.stack(output_channels, axis=0)\n",
    "                all_outputs.append(output_data)\n",
    "            \n",
    "            all_metadata.append({'exp': exp, 'time': t})\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_inputs = np.array(all_inputs)  # (N_samples, C, N_pixels)\n",
    "    if all_outputs:\n",
    "        all_outputs = np.array(all_outputs)\n",
    "    \n",
    "    # Create batches\n",
    "    n_samples = len(all_inputs)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, n_samples)\n",
    "        \n",
    "        batch_input = all_inputs[start_idx:end_idx]\n",
    "        batch_output = all_outputs[start_idx:end_idx] # if all_outputs else None\n",
    "        batch_meta = all_metadata[start_idx:end_idx]\n",
    "        \n",
    "        batches.append({\n",
    "            'input': batch_input,\n",
    "            'output': batch_output,\n",
    "            'metadata': batch_meta,\n",
    "            'batch_idx': i\n",
    "        })\n",
    "    \n",
    "    return batches, all_inputs, all_outputs\n",
    "\n",
    "\n",
    "def apply_log_transform(data, global_mean=None, global_std=None, eps=1e-12):\n",
    "    \"\"\"Apply log10 transform and normalization\"\"\"\n",
    "    data_log = np.log10(np.clip(data, 1e-15, None) + 1e-15)\n",
    "    \n",
    "    if global_mean is not None and global_std is not None:\n",
    "        data_scaled = (data_log - global_mean) / max(global_std, eps)\n",
    "    else:\n",
    "        data_scaled = (data_log - data_log.mean()) / max(data_log.std(), eps)\n",
    "    \n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9bc1b-0e63-4653-8143-c38833c828d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = ['BC', 'CH4', 'SO2', 'CO2']\n",
    "output_vars = [\"tas\", \"pr\"]\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Prepare batches manually\n",
    "print(\"Preparing batches...\")\n",
    "batches, all_inputs, all_outputs = prepare_manual_batches(\n",
    "    experiment_input_dict, \n",
    "    experiment_output_dict,\n",
    "    gas_vars= input_vars,\n",
    "    output_vars = output_vars,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
