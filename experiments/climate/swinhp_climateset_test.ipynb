{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d58bc5-ab81-490b-b0e0-ccf8ec4a7aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/x_tagty/equivariant-posteriors\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[1]  # or resolve explicitly\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeab09fa-b1a9-4511-b29c-2ba97ece151a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uv/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[Compute environment] paths: \n",
      "\u001b[92m[Paths] checkpoints: /proj/heal_pangu/eqp_climate/checkpoints (/proj/heal_pangu/eqp_climate/checkpoints)\u001b[0m\n",
      "\u001b[92m[Paths] locks: locks (/home/x_tagty/equivariant-posteriors/experiments/climate/locks)\u001b[0m\n",
      "\u001b[92m[Paths] distributed_requests: distributed_requests (/home/x_tagty/equivariant-posteriors/experiments/climate/distributed_requests)\u001b[0m\n",
      "\u001b[92m[Paths] artifacts: /proj/heal_pangu/eqp_climate/artifacts (/proj/heal_pangu/eqp_climate/artifacts)\u001b[0m\n",
      "\u001b[92m[Paths] datasets: /proj/heal_pangu/datasets (/proj/heal_pangu/datasets)\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "#import onnxruntime as ort\n",
    "\n",
    "from lib.train_dataclasses import TrainConfig\n",
    "from lib.train_dataclasses import TrainRun\n",
    "from lib.train_dataclasses import TrainEval\n",
    "from lib.train_dataclasses import OptimizerConfig\n",
    "from lib.train_dataclasses import ComputeConfig\n",
    "from lib.metric import create_metric\n",
    "\n",
    "from experiments.weather.models.swin_hp_pangu_pad import SwinHPPanguPadConfig\n",
    "\n",
    "from lib.distributed_trainer import distributed_train\n",
    "\n",
    "# from experiments.weather.data import DataHP\n",
    "from experiments.weather.data import DataHPConfig #, Climatology\n",
    "# from experiments.weather.models.swin_hp_pangu import SwinHPPangu\n",
    "\n",
    "# from lib.models.mlp import MLPConfig\n",
    "from lib.ddp import ddp_setup\n",
    "from lib.ensemble import create_ensemble_config\n",
    "from lib.ensemble import create_ensemble\n",
    "from lib.ensemble import request_ensemble\n",
    "from lib.ensemble import symlink_checkpoint_files\n",
    "from lib.ensemble import is_ensemble_serialized\n",
    "from lib.files import prepare_results\n",
    "\n",
    "from lib.data_factory import get_factory as get_dataset_factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54e392-4094-4600-ba10-18192576d8bd",
   "metadata": {},
   "source": [
    "## Load one data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342ff685-64be-4a05-81d5-817127bec9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from discretize hp once\n",
    "import healpix\n",
    "def interpolate_dh_to_hp(nside, variable: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Input is xr.DataArray \n",
    "    \"\"\"\n",
    "    \n",
    "    npix = healpix.nside2npix(nside)\n",
    "    hlong, hlat = healpix.pix2ang(nside, np.arange(0, npix, 1), lonlat=True, nest=True)\n",
    "    hlong = np.mod(hlong, 360)\n",
    "    xlong = xr.DataArray(hlong, dims=\"z\")\n",
    "    xlat = xr.DataArray(hlat, dims=\"z\")\n",
    "\n",
    "    xhp = variable.interp(lat=xlat, lon=xlong, kwargs={\"fill_value\": None})\n",
    "    hp_image = np.array(xhp.to_numpy(), dtype=np.float32) # ! removed to_array()\n",
    "    return hp_image\n",
    "\n",
    "def interpolate_dh_to_hp_output(nside, variable: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Input is xr.DataArray \n",
    "    \"\"\"\n",
    "    \n",
    "    npix = healpix.nside2npix(nside)\n",
    "    hlong, hlat = healpix.pix2ang(nside, np.arange(0, npix, 1), lonlat=True, nest=True)\n",
    "    hlong = np.mod(hlong, 360)\n",
    "    xlong = xr.DataArray(hlong, dims=\"z\")\n",
    "    xlat = xr.DataArray(hlat, dims=\"z\")\n",
    "\n",
    "    xhp = variable.interp(y=xlat, x=xlong, kwargs={\"fill_value\": None})\n",
    "    hp_image = np.array(xhp.to_numpy(), dtype=np.float32) # ! removed to_array()\n",
    "    return hp_image\n",
    "\n",
    "def e5_to_numpy_hp(e5xr, nside: int, normalized: bool):\n",
    "    \"\"\"\n",
    "    Input is class with xr.DataArray class variables\n",
    "    \"\"\"\n",
    "\n",
    "    hp_surface = interpolate_dh_to_hp(nside, e5xr.surface)\n",
    "    hp_upper = interpolate_dh_to_hp(nside, e5xr.upper)\n",
    "\n",
    "    if normalized:\n",
    "        stats = deserialize_dataset_statistics(nside)\n",
    "        hp_surface, hp_upper = normalize_sample(stats.item(), hp_surface, hp_upper)\n",
    "\n",
    "    return hp_surface, hp_upper\n",
    "\n",
    "def structure_data(ds, num_steps_to_plot):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for item in ds.data_vars:\n",
    "        print(item)\n",
    "\n",
    "    #num_steps_to_plot = 1\n",
    "    for timestep in range(num_steps_to_plot):\n",
    "        #print(timestep)\n",
    "        plot_gas_and_timestep(ds, timestep)\n",
    "    \n",
    "    arr = ds.to_array().values  # shape (variable, lat, lon) or (... depends on dataset)\n",
    "    print(arr.shape)\n",
    "    # If there is still a leading dim for 'variable', move to flatten consistently:\n",
    "    #arr = np.asarray(arr)\n",
    "    #return arr.ravel()\n",
    "    return None\n",
    "\n",
    "\n",
    "def first_nc_under(dirpath: str) -> str:\n",
    "    \"\"\"Return the first .nc file found under dirpath (recursive). Raises if none.\"\"\"\n",
    "    files = glob.glob(os.path.join(datapath, \"**\", \"*.nc\"), recursive=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .nc files under: {dirpath}\")\n",
    "    return files[0]\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ClimsetInputSample:\n",
    "    BC: xr.Dataset\n",
    "    CH4: xr.Dataset\n",
    "    CO2: xr.Dataset\n",
    "    SO2: xr.Dataset\n",
    "\n",
    "def get_output_data(path: str, mode: str):\n",
    "    nc_files = []\n",
    "    \n",
    "    if mode == 'train':\n",
    "        experiments = train_experiments\n",
    "    elif mode == 'test':\n",
    "        experiments = test_experiments\n",
    "        \n",
    "    for mod in models:\n",
    "\n",
    "        model_dir = os.path.join(path, mod)\n",
    "        ensembles = os.listdir(model_dir)\n",
    "\n",
    "        if total_ensembles == 1:\n",
    "            ensembles = ensembles[0]\n",
    "        \n",
    "        exp_counter = 0\n",
    "        for exp in experiments:\n",
    "            for var in variables:\n",
    "                var_dir = os.path.join(path, mod, ensembles, exp, var, '250_km/mon')\n",
    "                files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "                nc_files += files\n",
    "        \n",
    "            if exp_counter == 0:\n",
    "                dataset = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "        \n",
    "            else: #concatenate dataset in time dimension\n",
    "                other_experiment = xr.open_mfdataset(nc_files).compute().to_array().to_numpy()\n",
    "                dataset = np.concatenate((dataset, other_experiment), axis=1)\n",
    "                \n",
    "                \n",
    "            exp_counter += 1\n",
    "            \n",
    "        dataset = np.moveaxis(dataset, 0, 1)\n",
    "        print(dataset.shape)\n",
    "        dataset = dataset.reshape(dataset.shape[0], -1)\n",
    "        \n",
    "        # TODO: remove next line, only used for making quick tests\n",
    "        dataset = dataset[:, :1]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac91426-0d06-45cb-9f0d-9bc3aed71858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 34.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "86\n",
      "86\n",
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "datapath = \"/proj/heal_pangu/users/x_tagty/climateset\"\n",
    "input_dir = os.path.join(datapath, \"inputs\", \"input4mips\")\n",
    "target_dir = os.path.join(datapath, \"outputs\", \"CMIP6\")\n",
    "\n",
    "fire_type = 'all-fires'\n",
    "variables = ['tas']\n",
    "models = ['CAS-ESM2-0']\n",
    "train_experiments = [\"ssp585\", \"ssp126\", \"ssp370\"] \n",
    "test_experiments = [\"ssp245\"]\n",
    "input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']\n",
    "print(\"got here\")\n",
    "\n",
    "exp = train_experiments[0]\n",
    "gas_datasets = []\n",
    "\n",
    "BC = []\n",
    "CH4 = []\n",
    "CO2 = []\n",
    "SO2 = []\n",
    "for idx, gas in tqdm(enumerate(input_gases)):\n",
    "    var_dir = os.path.join(input_dir, exp, gas, '250_km', 'mon')\n",
    "    files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "\n",
    "    for f in files:\n",
    "        if gas == 'BC_sum' and fire_type in f:\n",
    "            BC.append(f)\n",
    "    for f in files:\n",
    "        if gas == 'CH4_sum' and fire_type in f:\n",
    "            CH4.append(f)\n",
    "    for f in files:\n",
    "        if gas == 'SO2_sum' and fire_type in f:\n",
    "            SO2.append(f)\n",
    "    for f in files:\n",
    "        if gas == 'CO2_sum':\n",
    "            CO2.append(f)\n",
    "\n",
    "print(len(BC))\n",
    "print(len(CH4))\n",
    "print(len(SO2))\n",
    "print(len(CO2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b7b560-ece6-4774-9b85-3ce4e5ffb195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gas_data[0] = /proj/heal_pangu/users/x_tagty/climateset/inputs/input4mips/ssp585/BC_sum/250_km/mon/2015/input4mips_ssp585_BC_all-fires_sum_250_km_mon_gn_2015.nc <class 'str'>\n",
      "<xarray.Dataset> Size: 666kB\n",
      "Dimensions:  (time: 12, lon: 144, lat: 96)\n",
      "Coordinates:\n",
      "  * time     (time) object 96B 2015-01-01 00:00:00 ... 2015-12-01 00:00:00\n",
      "  * lon      (lon) float64 1kB 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * lat      (lat) float64 768B -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0\n",
      "Data variables:\n",
      "    BC       (time, lat, lon) float32 664kB ...\n",
      "Attributes: (12/33)\n",
      "    CDI:                 Climate Data Interface version 2.0.4 (https://mpimet...\n",
      "    Conventions:         CF-1.6\n",
      "    activity_id:         input4MIPs\n",
      "    comment:             SSP harmonized, gridded emissions for IAMC-REMIND-MA...\n",
      "    contact:             Steven J. Smith (ssmith@pnnl.gov)\n",
      "    creation_date:       2018-06-21T03:03:44Z\n",
      "    ...                  ...\n",
      "    data_usage_tips:     Note that these are monthly average fluxes. Note tha...\n",
      "    reporting_unit:      Mass flux of BC, reported as carbon mass\n",
      "    tracking_id:         hdl:21.14100/e992502f-224b-40b2-ab70-ad7a2dcdda6e\n",
      "    remap_alg:           remapbil\n",
      "    orig_res:            50 km\n",
      "    CDO:                 Climate Data Operators version 2.0.4 (https://mpimet...\n",
      "gas_data[0] = /proj/heal_pangu/users/x_tagty/climateset/inputs/input4mips/ssp585/CH4_sum/250_km/mon/2015/input4mips_ssp585_CH4_all-fires_sum_250_km_mon_gn_2015.nc <class 'str'>\n",
      "<xarray.Dataset> Size: 666kB\n",
      "Dimensions:  (time: 12, lon: 144, lat: 96)\n",
      "Coordinates:\n",
      "  * time     (time) object 96B 2015-01-01 00:00:00 ... 2015-12-01 00:00:00\n",
      "  * lon      (lon) float64 1kB 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * lat      (lat) float64 768B -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0\n",
      "Data variables:\n",
      "    CH4      (time, lat, lon) float32 664kB ...\n",
      "Attributes: (12/33)\n",
      "    CDI:                 Climate Data Interface version 2.0.4 (https://mpimet...\n",
      "    Conventions:         CF-1.6\n",
      "    activity_id:         input4MIPs\n",
      "    comment:             SSP harmonized, gridded emissions for IAMC-REMIND-MA...\n",
      "    contact:             Steven J. Smith (ssmith@pnnl.gov)\n",
      "    creation_date:       2018-06-21T03:02:59Z\n",
      "    ...                  ...\n",
      "    data_usage_tips:     Note that these are monthly average fluxes. Note tha...\n",
      "    reporting_unit:      Mass flux of CH4\n",
      "    tracking_id:         hdl:21.14100/36280f18-7b34-464e-b8cf-6d3106a4b9b9\n",
      "    remap_alg:           remapbil\n",
      "    orig_res:            50 km\n",
      "    CDO:                 Climate Data Operators version 2.0.4 (https://mpimet...\n",
      "gas_data[0] = /proj/heal_pangu/users/x_tagty/climateset/inputs/input4mips/ssp585/SO2_sum/250_km/mon/2015/input4mips_ssp585_SO2_all-fires_sum_250_km_mon_gn_2015.nc <class 'str'>\n",
      "<xarray.Dataset> Size: 666kB\n",
      "Dimensions:  (time: 12, lon: 144, lat: 96)\n",
      "Coordinates:\n",
      "  * time     (time) object 96B 2015-01-01 00:00:00 ... 2015-12-01 00:00:00\n",
      "  * lon      (lon) float64 1kB 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * lat      (lat) float64 768B -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0\n",
      "Data variables:\n",
      "    SO2      (time, lat, lon) float32 664kB ...\n",
      "Attributes: (12/33)\n",
      "    CDI:                 Climate Data Interface version 2.0.4 (https://mpimet...\n",
      "    Conventions:         CF-1.6\n",
      "    activity_id:         input4MIPs\n",
      "    comment:             SSP harmonized, gridded emissions for IAMC-REMIND-MA...\n",
      "    contact:             Steven J. Smith (ssmith@pnnl.gov)\n",
      "    creation_date:       2018-06-21T03:08:29Z\n",
      "    ...                  ...\n",
      "    data_usage_tips:     Note that these are monthly average fluxes. Note tha...\n",
      "    reporting_unit:      Mass flux of SOx, reported as SO2\n",
      "    tracking_id:         hdl:21.14100/f4ee77fc-e901-4896-a9d3-9037ac2d1ec7\n",
      "    remap_alg:           remapbil\n",
      "    orig_res:            50 km\n",
      "    CDO:                 Climate Data Operators version 2.0.4 (https://mpimet...\n",
      "gas_data[0] = /proj/heal_pangu/users/x_tagty/climateset/inputs/input4mips/ssp585/CO2_sum/250_km/mon/2015/input4mips_ssp585_CO2_sum_250_km_mon_gn_2015.nc <class 'str'>\n",
      "<xarray.Dataset> Size: 666kB\n",
      "Dimensions:  (time: 12, lon: 144, lat: 96)\n",
      "Coordinates:\n",
      "  * time     (time) object 96B 2015-01-01 00:00:00 ... 2015-12-01 00:00:00\n",
      "  * lon      (lon) float64 1kB 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * lat      (lat) float64 768B -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0\n",
      "Data variables:\n",
      "    CO2      (time, lat, lon) float32 664kB ...\n",
      "Attributes: (12/34)\n",
      "    CDI:                 Climate Data Interface version 2.0.4 (https://mpimet...\n",
      "    Conventions:         CF-1.6\n",
      "    activity_id:         input4MIPs\n",
      "    comment:             SSP harmonized, gridded emissions for IAMC-REMIND-MA...\n",
      "    contact:             Steven J. Smith (ssmith@pnnl.gov)\n",
      "    creation_date:       2018-11-12T21:29:26Z\n",
      "    ...                  ...\n",
      "    tracking_id:         hdl:21.14100/5c3ff899-1a38-455a-8ab6-e3c633e317c2\n",
      "    variable_id:         CO2_em_AIR_anthro\n",
      "    NCO:                 netCDF Operators version 5.0.6 (Homepage = http://nc...\n",
      "    remap_alg:           remapbil\n",
      "    orig_res:            50 km\n",
      "    CDO:                 Climate Data Operators version 2.0.4 (https://mpimet...\n"
     ]
    }
   ],
   "source": [
    "# get to hp\n",
    "gas_dict = {'BC': BC, \n",
    "            'CH4': CH4,\n",
    "            'SO2': SO2, \n",
    "            'CO2': CO2\n",
    "           }\n",
    "\n",
    "timestep = 1\n",
    "data_timestep = []\n",
    "for key, gas_data in gas_dict.items():\n",
    "    print(\"gas_data[0] =\", gas_data[0], type(gas_data[0]))\n",
    "    with xr.open_dataset(gas_data[0]) as ds:\n",
    "        print(ds)\n",
    "        data_grid = ds[key].isel(time=timestep)\n",
    "        data_timestep.append(data_grid)\n",
    "\n",
    "\n",
    "test_sample = ClimsetInputSample(\n",
    "    BC = data_timestep[0],\n",
    "    CH4 = data_timestep[1],\n",
    "    SO2 = data_timestep[2],\n",
    "    CO2 = data_timestep[3])\n",
    "\n",
    "nside = 32\n",
    "BC_hp = interpolate_dh_to_hp(nside, test_sample.BC)\n",
    "CH4_hp = interpolate_dh_to_hp(nside, test_sample.CH4)\n",
    "SO2_hp = interpolate_dh_to_hp(nside, test_sample.SO2)\n",
    "CO2_hp = interpolate_dh_to_hp(nside, test_sample.CO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31bbaf-20e8-43da-b625-60323ce73bd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load multiple samples, fix code with dask thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc16fdc-f315-4093-bb6a-dd8978a88ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires to install eofs and gpytorch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import gpytorch\n",
    "import os\n",
    "import glob\n",
    "#from eofs.xarray import Eof\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict, Optional, List, Callable, Tuple, Union\n",
    "\n",
    "#import wandb\n",
    "#from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d69684-0223-468c-ace8-49f6e4700ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_paths(exp, input_dir, fire_type):\n",
    "    input_gasses = {\n",
    "    \"BC\":  \"BC_sum\",\n",
    "    \"CH4\": \"CH4_sum\",\n",
    "    \"SO2\": \"SO2_sum\",\n",
    "    \"CO2\": \"CO2_sum\",\n",
    "    }\n",
    "    gas_files = {g: [] for g in gas_patterns}\n",
    "    for gas, folder_name in input_gasses.items():\n",
    "        var_dir = os.path.join(input_dir, exp, folder_name, \"250_km\", \"mon\")\n",
    "        files = glob.glob(var_dir + \"/**/*.nc\", recursive=True)\n",
    "    \n",
    "        for f in files:\n",
    "            if folder_name in f and (gas == \"CO2\" or fire_type in f): # CO2 does not have fire_type\n",
    "                gas_files[gas].append(f)\n",
    "    #print(gas_files[\"BC\"])\n",
    "    # Check same len\n",
    "    for k, v in gas_files.items():\n",
    "        print(k, len(v))\n",
    "\n",
    "    return gas_files\n",
    "\n",
    "def get_output_paths(exp, target_dir, mod, ensembles, variables):\n",
    "    var_file_dict = {v: [] for v in variables}\n",
    "    for var in variables:\n",
    "        var_dir = os.path.join(target_dir, mod, ensembles, exp, var, '250_km', 'mon')\n",
    "        var_files = glob.glob(var_dir + '/**/*.nc', recursive=True)\n",
    "        #var_file_dict[var].append(var_files)\n",
    "        for f in var_files:\n",
    "            var_file_dict[var].append(f)\n",
    "\n",
    "    return var_file_dict\n",
    "\n",
    "def get_hp_dataset(files : dict, nside : int, output : bool = False):\n",
    "    hp_gas_dict = {}\n",
    "    #print(files)\n",
    "    for var, var_files in files.items():\n",
    "        #print(var, var_files)\n",
    "        ds = xr.open_mfdataset(var_files, concat_dim=\"time\", combine=\"nested\")\n",
    "        ds = ds.sortby(\"time\")\n",
    "\n",
    "        arr = ds.to_array().squeeze(\"variable\") # remove var dim since 1\n",
    "\n",
    "        if output:\n",
    "            hp = interpolate_dh_to_hp_output(nside, arr)\n",
    "        else:\n",
    "            hp = interpolate_dh_to_hp(nside, arr)\n",
    "        #print(type(hp))\n",
    "        hp_gas_dict[var] = hp\n",
    "\n",
    "    return hp_gas_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4889bd3-c38e-43c9-ab79-bd7299db8279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC 86\n",
      "CH4 86\n",
      "SO2 86\n",
      "CO2 86\n",
      "got input paths\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "chunk manager 'dask' is not available. Please make sure 'dask' is installed and importable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m input_paths = get_input_paths(exp, input_dir, fire_type)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgot input paths\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m hp_input_dict = \u001b[43mget_hp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mhp input done\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# TODO : Add concatenate or similar if multiple experiments\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mget_hp_dataset\u001b[39m\u001b[34m(files, nside, output)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m#print(files)\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var, var_files \u001b[38;5;129;01min\u001b[39;00m files.items():\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m#print(var, var_files)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     ds = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnested\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     ds = ds.sortby(\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m     arr = ds.to_array().squeeze(\u001b[33m\"\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# remove var dim since 1\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uv/.venv/lib/python3.12/site-packages/xarray/backends/api.py:1635\u001b[39m, in \u001b[36mopen_mfdataset\u001b[39m\u001b[34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[39m\n\u001b[32m   1632\u001b[39m     open_ = open_dataset\n\u001b[32m   1633\u001b[39m     getattr_ = \u001b[38;5;28mgetattr\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1635\u001b[39m datasets = [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths1d]\n\u001b[32m   1636\u001b[39m closers = [getattr_(ds, \u001b[33m\"\u001b[39m\u001b[33m_close\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[32m   1637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uv/.venv/lib/python3.12/site-packages/xarray/backends/api.py:693\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    686\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    687\u001b[39m backend_ds = backend.open_dataset(\n\u001b[32m    688\u001b[39m     filename_or_obj,\n\u001b[32m    689\u001b[39m     drop_variables=drop_variables,\n\u001b[32m    690\u001b[39m     **decoders,\n\u001b[32m    691\u001b[39m     **kwargs,\n\u001b[32m    692\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m ds = \u001b[43m_dataset_from_backend_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uv/.venv/lib/python3.12/site-packages/xarray/backends/api.py:402\u001b[39m, in \u001b[36m_dataset_from_backend_dataset\u001b[39m\u001b[34m(backend_ds, filename_or_obj, engine, chunks, cache, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, **extra_tokens)\u001b[39m\n\u001b[32m    400\u001b[39m     ds = backend_ds\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     ds = \u001b[43m_chunk_ds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m ds.set_close(backend_ds._close)\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# Ensure source filename always stored in dataset object\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uv/.venv/lib/python3.12/site-packages/xarray/backends/api.py:350\u001b[39m, in \u001b[36m_chunk_ds\u001b[39m\u001b[34m(backend_ds, filename_or_obj, engine, chunks, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, **extra_tokens)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chunk_ds\u001b[39m(\n\u001b[32m    340\u001b[39m     backend_ds,\n\u001b[32m    341\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **extra_tokens,\n\u001b[32m    349\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     chunkmanager = \u001b[43mguess_chunkmanager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# TODO refactor to move this dask-specific logic inside the DaskManager class\u001b[39;00m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunkmanager, DaskManager):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uv/.venv/lib/python3.12/site-packages/xarray/namedarray/parallelcompat.py:116\u001b[39m, in \u001b[36mguess_chunkmanager\u001b[39m\u001b[34m(manager)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(manager, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_chunkmanagers \u001b[38;5;129;01mand\u001b[39;00m manager \u001b[38;5;129;01min\u001b[39;00m KNOWN_CHUNKMANAGERS:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    117\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchunk manager \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is not available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Please make sure \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKNOWN_CHUNKMANAGERS[manager]\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m and importable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         )\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(available_chunkmanagers) == \u001b[32m0\u001b[39m:\n\u001b[32m    122\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    123\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mno chunk managers available. Try installing `dask` or another package\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m that provides a chunk manager.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m         )\n",
      "\u001b[31mImportError\u001b[39m: chunk manager 'dask' is not available. Please make sure 'dask' is installed and importable."
     ]
    }
   ],
   "source": [
    "datapath = \"/proj/heal_pangu/users/x_tagty/climateset\"\n",
    "input_dir = os.path.join(datapath, \"inputs\", \"input4mips\")\n",
    "target_dir = os.path.join(datapath, \"outputs\", \"CMIP6\")\n",
    "\n",
    "fire_type = 'all-fires'\n",
    "#variables = ['tas']\n",
    "\n",
    "mod = 'CAS-ESM2-0'\n",
    "ensembles = 'r3i1p1f1'\n",
    "train_experiments = [\"ssp585\", \"ssp126\", \"ssp370\"]\n",
    "experiments = [train_experiments[0]]\n",
    "#input_gases = ['BC_sum', 'CH4_sum', 'CO2_sum', 'SO2_sum']\n",
    "gas_patterns = {\n",
    "    \"BC\":  \"BC_sum\",\n",
    "    \"CH4\": \"CH4_sum\",\n",
    "    \"SO2\": \"SO2_sum\",\n",
    "    \"CO2\": \"CO2_sum\",\n",
    "}\n",
    "\n",
    "nside = 32\n",
    "for exp in experiments:\n",
    "    input_paths = get_input_paths(exp, input_dir, fire_type)\n",
    "    print(\"got input paths\")\n",
    "    hp_input_dict = get_hp_dataset(input_paths, nside)\n",
    "    print(\"hp input done\")\n",
    "    # TODO : Add concatenate or similar if multiple experiments\n",
    "    \n",
    "    print(\"checking outputs\")\n",
    "    output_paths = get_output_paths(exp, target_dir, mod, ensembles, variables)\n",
    "    print(\"got output paths\")\n",
    "    hp_target_dict = get_hp_dataset(output_paths, nside, output=True)\n",
    "    # TODO : Add concatenate or similar  if multiple experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b45ec-6319-4b4b-a7b5-aff304e96629",
   "metadata": {},
   "source": [
    "## Create / adapt model\n",
    "\n",
    "Make sure four channels, one for each gas\n",
    "Send in one timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f8f13-792d-4b3f-9a4a-aa47fe16c8da",
   "metadata": {},
   "source": [
    "### Create one timestep sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7aba102-e382-4ce7-bb3a-d10862043ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 12288)\n"
     ]
    }
   ],
   "source": [
    "input_test_array2 = np.array([BC_hp, CH4_hp, SO2_hp, CO2_hp])\n",
    "print(input_test_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f1c6f4-98f6-47df-b1f4-79c0a3b53a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# from lib.models.healpix.swin_hp_transformer import SwinHPTransformerConfig\n",
    "\n",
    "# from lib.models.mlp import MLPConfig\n",
    "from lib.serialize_human import serialize_human\n",
    "from lib.compute_env import env\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import healpix\n",
    "import chealpix\n",
    "import xarray as xr\n",
    "import experiments.weather.cdsmontly as cdstest\n",
    "import experiments.weather.masks.masks as masks\n",
    "\n",
    "\n",
    "def numpy_to_xds(np_array, xds_template):\n",
    "    transformed_ds = xr.Dataset()\n",
    "    for i, var_name in enumerate(xds_template.data_vars):\n",
    "        transformed_ds[var_name] = xr.DataArray(\n",
    "            np_array[i], dims=xds_template.dims, coords=xds_template.coords\n",
    "        )\n",
    "    return transformed_ds\n",
    "\n",
    "\n",
    "ERA5_START_YEAR_TRAINING = 2007\n",
    "ERA5_END_YEAR_TRAINING = 2017\n",
    "ERA5_START_YEAR_TEST = 2019\n",
    "ERA5_END_YEAR_TEST = 2019\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataHPConfig:\n",
    "    nside: int = 64\n",
    "    version: int = 10\n",
    "    driscoll_healy: bool = False\n",
    "     # cache: bool = True\n",
    "    normalized: bool = True\n",
    "    # start_year: int = 2007\n",
    "    # end_year: int = 2017\n",
    "    lead_time_days: int = 1\n",
    "\n",
    "    def short_name(self):\n",
    "        return f\"era5_{self.start_year}_{self.end_year}\"\n",
    "\n",
    "    def serialize_human(self):\n",
    "        return serialize_human(self.__dict__)\n",
    "\n",
    "    def custom_dict(self):\n",
    "        serialize_dict = copy.deepcopy(self.__dict__)\n",
    "        if self.start_year == 2007 and self.end_year == 2017:\n",
    "            del serialize_dict[\"start_year\"]\n",
    "            del serialize_dict[\"end_year\"]\n",
    "        del serialize_dict[\"lead_time_days\"]\n",
    "        return serialize_dict\n",
    "\n",
    "    def statistics_name(self):\n",
    "        keys = sorted(self.__dict__.keys())\n",
    "        keys.remove(\"cache\")\n",
    "        keys.remove(\"normalized\")\n",
    "        keys.remove(\"lead_time_days\")\n",
    "        keys.remove(\"end_year\")\n",
    "        return \"_\".join([f\"{key}_{self.__dict__[key]}\" for key in keys])\n",
    "\n",
    "    def cache_name(self):\n",
    "        keys = sorted(self.__dict__.keys())\n",
    "        keys.remove(\"cache\")\n",
    "        keys.remove(\"lead_time_days\")\n",
    "        keys.remove(\"end_year\")\n",
    "        return \"_\".join([f\"{key}_{self.__dict__[key]}\" for key in keys])\n",
    "\n",
    "    def validation(self):\n",
    "        ret = copy.deepcopy(self)\n",
    "        ret.start_year = ERA5_START_YEAR_TEST\n",
    "        ret.end_year = ERA5_END_YEAR_TEST\n",
    "        return ret\n",
    "\n",
    "    def as_hp(self):\n",
    "        ret = copy.deepcopy(self)\n",
    "        ret.driscoll_healy = False\n",
    "        return ret\n",
    "\n",
    "    def with_lead_time_days(self, days: int):\n",
    "        ret = copy.deepcopy(self)\n",
    "        ret.lead_time_days = days\n",
    "        return ret\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSpecHP:\n",
    "    nside: int\n",
    "    n_surface: int\n",
    "    n_upper: int\n",
    "\n",
    "\n",
    "def days_between_years(start_year: int, end_year: int) -> int:\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    return (end_date - start_date).days\n",
    "\n",
    "\n",
    "def days_from_start_year(start_year: int, year: int, month: int, day: int) -> int:\n",
    "    start_date = datetime(start_year, 1, 1)  # Start of the start year\n",
    "    specific_date = datetime(year, month, day)  # Specific date\n",
    "    return (specific_date - start_date).days\n",
    "\n",
    "\n",
    "def day_index_to_datetime(day_index: int, start_year: int, end_year: int):\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    target_date = start_date + timedelta(days=day_index)\n",
    "    return target_date\n",
    "\n",
    "\n",
    "def day_index_to_era5_config(\n",
    "    day_index: int, start_year: int, end_year: int\n",
    ") -> cdstest.ERA5SampleConfig:\n",
    "    target_date = day_index_to_datetime(day_index, start_year, end_year)\n",
    "    return cdstest.ERA5SampleConfig(\n",
    "        year=target_date.strftime(\"%Y\"),\n",
    "        month=target_date.strftime(\"%m\"),\n",
    "        day=target_date.strftime(\"%d\"),\n",
    "        time=\"00:00:00\",\n",
    "    )\n",
    "\n",
    "\n",
    "def day_index_to_climatology_indices(day_index, start_year, end_year):\n",
    "    day_config = day_index_to_era5_config(day_index, start_year, end_year)\n",
    "    indices = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        indices.append(\n",
    "            days_from_start_year(\n",
    "                start_year, year, int(day_config.month), int(day_config.day)\n",
    "            )\n",
    "        )\n",
    "    return indices\n",
    "\n",
    "\n",
    "class Climatology(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_config: DataHPConfig):\n",
    "        self.ds = DataHP(data_config)\n",
    "        climate_config = copy.deepcopy(data_config)\n",
    "        climate_config.start_year = ERA5_START_YEAR_TRAINING\n",
    "        climate_config.end_year = ERA5_END_YEAR_TRAINING\n",
    "        self.ds_climate = DataHP(climate_config)\n",
    "        self.climate_config = climate_config\n",
    "        self.config = data_config\n",
    "\n",
    "    def get_meta(self):\n",
    "        return self.ds.get_meta()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cache_path = self.ds_climate.get_cache_dir() / \"climate\"\n",
    "        climate_indices = day_index_to_climatology_indices(\n",
    "            index, self.climate_config.start_year, self.climate_config.end_year\n",
    "        )\n",
    "        indices_str = \"_\".join(map(str, climate_indices))\n",
    "        fs_cache_path = cache_path / f\"{indices_str}\"\n",
    "        fs_cache_path_tmp = cache_path / f\"{indices_str}_constructing\"\n",
    "        names = dict(\n",
    "            climate_target_surface=\"climate_target_surface.npy\",\n",
    "            climate_target_upper=\"climate_target_upper.npy\",\n",
    "        )\n",
    "        item_dict = dict(\n",
    "            sample_id=index,\n",
    "        )\n",
    "        if fs_cache_path.is_dir() and self.config.cache:\n",
    "            print(\"Climate cache\")\n",
    "            item_dict.update(self.ds[index])\n",
    "            for key, filename in names.items():\n",
    "                item_dict[key] = np.load(fs_cache_path / filename).astype(np.float32)\n",
    "        else:\n",
    "            print(\"Climate hydrating cache\")\n",
    "            accum_dict = dict()\n",
    "            for cidx in climate_indices:\n",
    "                data_dict = self.ds_climate[cidx]\n",
    "                keys = [\"target_surface\", \"target_upper\"]\n",
    "                for key in keys:\n",
    "                    if key not in accum_dict:\n",
    "                        accum_dict[key] = data_dict[key].astype(np.float64)\n",
    "                    else:\n",
    "                        accum_dict[key] += data_dict[key].astype(np.float64)\n",
    "            for key in accum_dict.keys():\n",
    "                accum_dict[key] /= len(climate_indices)\n",
    "\n",
    "            item_dict.update(self.ds[index])\n",
    "            item_dict[\"climate_target_surface\"] = accum_dict[\"target_surface\"]\n",
    "            item_dict[\"climate_target_upper\"] = accum_dict[\"target_upper\"]\n",
    "            if self.config.cache:\n",
    "                fs_cache_path_tmp.mkdir(parents=True, exist_ok=True)\n",
    "                for key, filename in names.items():\n",
    "                    np.save(fs_cache_path_tmp / filename, item_dict[key])\n",
    "\n",
    "                if not fs_cache_path.is_dir():\n",
    "                    shutil.move(fs_cache_path_tmp, fs_cache_path)\n",
    "\n",
    "        return item_dict\n",
    "\n",
    "\n",
    "def interpolate_dh_to_hp(nside, variable: xr.DataArray):\n",
    "    npix = healpix.nside2npix(nside)\n",
    "    hlong, hlat = healpix.pix2ang(nside, np.arange(0, npix, 1), lonlat=True, nest=True)\n",
    "    hlong = np.mod(hlong, 360)\n",
    "    xlong = xr.DataArray(hlong, dims=\"z\")\n",
    "    xlat = xr.DataArray(hlat, dims=\"z\")\n",
    "\n",
    "    xhp = variable.interp(latitude=xlat, longitude=xlong, kwargs={\"fill_value\": None})\n",
    "    hp_image = np.array(xhp.to_array().to_numpy(), dtype=np.float32)\n",
    "    return hp_image\n",
    "\n",
    "\n",
    "def e5_to_numpy_hp(e5xr, nside: int, normalized: bool):\n",
    "\n",
    "    hp_surface = interpolate_dh_to_hp(nside, e5xr.surface)\n",
    "    hp_upper = interpolate_dh_to_hp(nside, e5xr.upper)\n",
    "\n",
    "    if normalized:\n",
    "        stats = deserialize_dataset_statistics(nside)\n",
    "        hp_surface, hp_upper = normalize_sample(stats.item(), hp_surface, hp_upper)\n",
    "\n",
    "    return hp_surface, hp_upper\n",
    "\n",
    "\n",
    "def batch_to_weatherbench2(\n",
    "    input_batch, output_batch, nside: int, normalized: bool, lead_days: int = 1\n",
    "):\n",
    "    xds = numpy_hp_to_e5(\n",
    "        output_batch[\"logits_surface\"],\n",
    "        output_batch[\"logits_upper\"],\n",
    "        times=input_batch[\"time\"],\n",
    "        nside=nside,\n",
    "        normalized=normalized,\n",
    "        lead_days=lead_days,\n",
    "    )\n",
    "    return xds\n",
    "\n",
    "\n",
    "def numpy_hp_to_e5(\n",
    "    hp_surface, hp_upper, times, nside: int, normalized: bool, lead_days: int = 1\n",
    "):\n",
    "    if torch.isnan(hp_surface).any():\n",
    "        print(\"NaNs!\")\n",
    "    if torch.isnan(hp_upper).any():\n",
    "        print(\"NaNs!\")\n",
    "\n",
    "    if normalized:\n",
    "        stats = deserialize_dataset_statistics(nside)\n",
    "        hp_surface, hp_upper = denormalize_sample(stats.item(), hp_surface, hp_upper)\n",
    "\n",
    "    def regrid_to_original(hp_data, dims):\n",
    "        xhp = xr.DataArray(hp_data, dims=dims)\n",
    "        lat = np.linspace(-90.0, 90.0, 721, endpoint=True)\n",
    "        lon = np.linspace(0.0, 360.0, 1440, endpoint=False)\n",
    "        lat2, lon2 = np.meshgrid(lat, lon, indexing=\"ij\")\n",
    "        idxs = healpix.ang2pix(nside, lon2, lat2, nest=False, lonlat=True)\n",
    "        idxs = chealpix.ring2nest(nside, idxs)\n",
    "        idxhp = xr.DataArray(\n",
    "            idxs,\n",
    "            dims=[\"latitude\", \"longitude\"],\n",
    "            coords={\"latitude\": lat, \"longitude\": lon},\n",
    "        )\n",
    "        return xhp.interp(z=idxhp, kwargs={\"fill_value\": \"extrapolate\"})\n",
    "\n",
    "    surface = regrid_to_original(\n",
    "        np.expand_dims(hp_surface, 1),\n",
    "        dims=(\"time\", \"prediction_timedelta\", \"variable\", \"z\"),\n",
    "    )\n",
    "    upper = regrid_to_original(\n",
    "        np.expand_dims(hp_upper, 1),\n",
    "        dims=(\"time\", \"prediction_timedelta\", \"variable\", \"level\", \"z\"),\n",
    "    )\n",
    "\n",
    "    if surface.isnull().any():\n",
    "        print(\"surface NaN after regrid\")\n",
    "    if upper.isnull().any():\n",
    "        print(\"upper NaN after regrid\")\n",
    "\n",
    "    surface = surface.assign_coords(\n",
    "        {\n",
    "            \"time\": [np.datetime64(t) for t in times],\n",
    "            \"prediction_timedelta\": [np.timedelta64(timedelta(days=lead_days))],\n",
    "        }\n",
    "    )\n",
    "    upper = upper.assign_coords(\n",
    "        {\n",
    "            \"time\": [np.datetime64(t) for t in times],\n",
    "            \"prediction_timedelta\": [np.timedelta64(timedelta(days=lead_days))],\n",
    "            \"level\": np.array(\n",
    "                [\n",
    "                    1000,\n",
    "                    925,\n",
    "                    850,\n",
    "                    700,\n",
    "                    600,\n",
    "                    500,\n",
    "                    400,\n",
    "                    300,\n",
    "                    250,\n",
    "                    200,\n",
    "                    150,\n",
    "                    100,\n",
    "                    50,\n",
    "                ],\n",
    "                dtype=np.int32,\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    surface_ds = surface.to_dataset(\"variable\").rename_vars(\n",
    "        {\n",
    "            0: \"mean_sea_level_pressure\",\n",
    "            1: \"10m_u_component_of_wind\",\n",
    "            2: \"10m_v_component_of_wind\",\n",
    "            3: \"2m_temperature\",\n",
    "        }\n",
    "    )\n",
    "    upper_ds = upper.to_dataset(\"variable\").rename_vars(\n",
    "        {\n",
    "            0: \"geopotential\",\n",
    "            1: \"specific_humidity\",\n",
    "            2: \"temperature\",\n",
    "            3: \"u_component_of_wind\",\n",
    "            4: \"v_component_of_wind\",\n",
    "        }\n",
    "    )\n",
    "    # e5xr = xr.Dataset({\"surface\": surface, \"upper\": upper})\n",
    "    # Identify misaligned coordinates\n",
    "\n",
    "    final = xr.merge(\n",
    "        [surface_ds.drop_vars(\"z\"), upper_ds.drop_vars(\"z\")],\n",
    "    )  # surface, upper\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "class DataHP(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_config: DataHPConfig):\n",
    "        self.config = data_config\n",
    "        # self.masks = masks.load_mask_hp(data_config.nside)\n",
    "\n",
    "    @staticmethod\n",
    "    def data_spec(config: DataHPConfig):\n",
    "        return DataSpecHP(nside=config.nside, n_surface=4, n_upper=5)\n",
    "\n",
    "    def e5_to_numpy(self, e5xr):\n",
    "        if self.config.driscoll_healy:\n",
    "            return self.e5_to_numpy_dh(e5xr)\n",
    "        else:\n",
    "            return e5_to_numpy_hp(e5xr, self.config.nside, self.config.normalized)\n",
    "\n",
    "    def dh_resolution(self):\n",
    "        n_pixels = healpix.nside2npix(self.config.nside)\n",
    "        # np_dh = w * h / 2^(2n)\n",
    "        # 2^(2n) = w * h / np_dh\n",
    "        # 2n = log2(w*h/np_dh)\n",
    "        # n = log2(w*h/np_dh) / 2\n",
    "        scale_factor = math.log2(1440 * 721 / n_pixels) / 2\n",
    "        lon = math.ceil(1440 / 2**scale_factor)\n",
    "        lat = math.ceil(721 / 2**scale_factor)\n",
    "        return dict(lat=lat, lon=lon)\n",
    "\n",
    "    def e5_to_numpy_dh(self, e5xr):\n",
    "        n_pixels = healpix.nside2npix(self.config.nside)\n",
    "        # np_dh = w * h / 2^(2n)\n",
    "        # 2^(2n) = w * h / np_dh\n",
    "        # 2n = log2(w*h/np_dh)\n",
    "        # n = log2(w*h/np_dh) / 2\n",
    "        scale_factor = math.log2(1440 * 721 / n_pixels) / 2\n",
    "        lon = math.ceil(1440 / 2**scale_factor)\n",
    "        lat = math.ceil(721 / 2**scale_factor)\n",
    "\n",
    "        new_lon = np.linspace(\n",
    "            e5xr.surface.longitude[0], e5xr.surface.longitude[-1], lon\n",
    "        )\n",
    "        new_lat = np.linspace(e5xr.surface.latitude[0], e5xr.surface.latitude[-1], lat)\n",
    "\n",
    "        def interpolate(variable: xr.DataArray):\n",
    "            xhp = variable.interp(\n",
    "                latitude=new_lat, longitude=new_lon, kwargs={\"fill_value\": None}\n",
    "            )\n",
    "            np_image = np.array(xhp.to_array().to_numpy(), dtype=np.float32)\n",
    "            return np_image\n",
    "\n",
    "        dh_surface = interpolate(e5xr.surface)\n",
    "        dh_upper = interpolate(e5xr.upper)\n",
    "        if self.config.normalized:\n",
    "            stats = deserialize_dataset_statistics(self.config.nside)\n",
    "            dh_surface, dh_upper = normalize_sample(stats.item(), dh_surface, dh_upper)\n",
    "\n",
    "        return dh_surface, dh_upper\n",
    "\n",
    "    def get_driscoll_healy(self, idx):\n",
    "        e5sc = cdstest.ERA5SampleConfig(\n",
    "            year=\"1999\", month=\"01\", day=\"01\", time=\"00:00:00\"\n",
    "        )\n",
    "        e5s = cdstest.get_era5_sample(e5sc)\n",
    "        e5_target_config = cdstest.ERA5SampleConfig(\n",
    "            year=\"1999\", month=\"01\", day=\"01\", time=\"03:00:00\"\n",
    "        )\n",
    "        e5target = cdstest.get_era5_sample(e5_target_config)\n",
    "        return e5s, e5target\n",
    "\n",
    "    def get_template_e5s(self):\n",
    "        e5sc = cdstest.ERA5SampleConfig(\n",
    "            year=\"2007\", month=\"01\", day=\"01\", time=\"00:00:00\"\n",
    "        )\n",
    "        e5s = cdstest.get_era5_sample(e5sc)\n",
    "        return e5s\n",
    "\n",
    "    def get_meta(self):\n",
    "        temp = self.get_template_e5s()\n",
    "\n",
    "        def get_metas(variable):\n",
    "            names = [str(var) for var in variable.data_vars.variables]\n",
    "            long_names = [\n",
    "                variable.data_vars.variables[var].attrs[\"long_name\"] for var in names\n",
    "            ]\n",
    "            units = [variable.data_vars.variables[var].attrs[\"units\"] for var in names]\n",
    "            return dict(\n",
    "                names=names,\n",
    "                long_names=long_names,\n",
    "                units=units,\n",
    "            )\n",
    "\n",
    "        surface_metas = get_metas(temp.surface)\n",
    "        upper_metas = get_metas(temp.upper)\n",
    "\n",
    "        return dict(surface=surface_metas, upper=upper_metas)\n",
    "\n",
    "    def get_cache_dir(self):\n",
    "        return env().paths.datasets / self.config.cache_name()\n",
    "\n",
    "    def get_statistics_path(self):\n",
    "        return env().paths.datasets / f\"{self.config.statistics_name()}.npy\"\n",
    "\n",
    "    def get_old_cache_dir(self):\n",
    "        if self.config.driscoll_healy:\n",
    "            return (\n",
    "                env().paths.datasets\n",
    "                / f\"era5_lite_np_cache_normalized_{self.config.normalized}_nside_{self.config.nside}_dh\"\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                env().paths.datasets\n",
    "                / f\"era5_lite_np_cache_normalized_{self.config.normalized}_nside_{self.config.nside}\"\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.config.lead_time_days == 1:\n",
    "            return self._get_24h(idx)\n",
    "        else:\n",
    "            sample = self._get_24h(idx)\n",
    "            target = self._get_24h(idx + self.config.lead_time_days - 1)\n",
    "            sample[\"target_surface\"] = target[\"target_surface\"]\n",
    "            sample[\"target_upper\"] = target[\"target_upper\"]\n",
    "            sample[\"prediction_timedelta_hours\"] = 24 * self.config.lead_time_days\n",
    "            return sample\n",
    "\n",
    "    def _get_24h(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise StopIteration()\n",
    "        fs_cache_path = self.get_cache_dir() / f\"{idx}\"\n",
    "        fs_cache_path_tmp = self.get_cache_dir() / f\"{idx}_constructing\"\n",
    "        names = dict(\n",
    "            input_surface=\"surface.npy\",\n",
    "            input_upper=\"upper.npy\",\n",
    "            target_surface=\"target_surface.npy\",\n",
    "            target_upper=\"target_upper.npy\",\n",
    "        )\n",
    "        item_dict = dict(\n",
    "            sample_id=idx,\n",
    "            time=np.datetime_as_string(\n",
    "                np.datetime64(\n",
    "                    day_index_to_datetime(\n",
    "                        idx, self.config.start_year, self.config.end_year\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            prediction_timedelta_hours=24,\n",
    "        )\n",
    "        done = False\n",
    "        while not done:\n",
    "            try:\n",
    "                fs_cache_path.is_dir()\n",
    "                done = True\n",
    "            except OSError:\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        if fs_cache_path.is_dir() and self.config.cache:\n",
    "            # print(\"Loading from cache\")\n",
    "            for key, filename in names.items():\n",
    "                item_dict[key] = np.load(fs_cache_path / filename).astype(np.float32)\n",
    "        else:\n",
    "            e5s_input_config = day_index_to_era5_config(\n",
    "                idx, self.config.start_year, self.config.end_year\n",
    "            )\n",
    "            e5s_target_config = cdstest.add_timedelta(e5s_input_config, days=1)\n",
    "            # print(\"Get ERA5 sample\")\n",
    "            e5s = cdstest.get_era5_sample(e5s_input_config)\n",
    "            e5target = cdstest.get_era5_sample(e5s_target_config)\n",
    "            # print(\"Get ERA5 sample done\")\n",
    "            hp_surface, hp_upper = self.e5_to_numpy(e5s)\n",
    "            hp_target_surface, hp_target_upper = self.e5_to_numpy(e5target)\n",
    "            # print(\"To numpy done\")\n",
    "            data_dict = dict(\n",
    "                input_surface=hp_surface.astype(np.float32),\n",
    "                input_upper=hp_upper.astype(np.float32),\n",
    "                target_surface=hp_target_surface.astype(np.float32),\n",
    "                target_upper=hp_target_upper.astype(np.float32),\n",
    "            )\n",
    "            if self.config.cache:\n",
    "                fs_cache_path_tmp.mkdir(parents=True, exist_ok=True)\n",
    "                for key, filename in names.items():\n",
    "                    np.save(fs_cache_path_tmp / filename, data_dict[key])\n",
    "\n",
    "                open(fs_cache_path_tmp / \"era5config_input.json\", \"w\").write(\n",
    "                    json.dumps(e5s_input_config.__dict__, indent=2)\n",
    "                )\n",
    "                open(fs_cache_path_tmp / \"era5config_target.json\", \"w\").write(\n",
    "                    json.dumps(e5s_target_config.__dict__, indent=2)\n",
    "                )\n",
    "\n",
    "                if not fs_cache_path.is_dir():\n",
    "                    shutil.move(fs_cache_path_tmp, fs_cache_path)\n",
    "            # print(\"Write done\")\n",
    "\n",
    "            item_dict.update(data_dict)\n",
    "\n",
    "        item_dict[\"masks\"] = masks.load_mask_hp(self.config.nside)\n",
    "\n",
    "        return item_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return days_between_years(self.config.start_year, self.config.end_year)\n",
    "\n",
    "\n",
    "def deserialize_dataset_statistics(nside):\n",
    "    ds = DataHP(DataHPConfig(nside=nside))\n",
    "    return np.load(ds.get_statistics_path(), allow_pickle=True)\n",
    "\n",
    "\n",
    "def _fix_dataset_statistics(nside):\n",
    "    stats = deserialize_dataset_statistics(nside).item()\n",
    "    std_surface = np.sqrt(\n",
    "        stats[\"mean_x2_surface\"].astype(np.float64)\n",
    "        - (stats[\"mean_surface\"].astype(np.float64)) ** 2\n",
    "    ).astype(np.float32)\n",
    "    std_upper = np.sqrt(\n",
    "        stats[\"mean_x2_upper\"].astype(np.float64)\n",
    "        - (stats[\"mean_upper\"].astype(np.float64)) ** 2\n",
    "    ).astype(np.float32)\n",
    "    statistics_dict = dict(\n",
    "        mean_surface=stats[\"mean_surface\"],\n",
    "        mean_upper=stats[\"mean_upper\"],\n",
    "        mean_x2_surface=stats[\"mean_x2_surface\"],\n",
    "        mean_x2_upper=stats[\"mean_x2_upper\"],\n",
    "        std_surface=std_surface,\n",
    "        std_upper=std_upper,\n",
    "        n_samples=stats[\"n_samples\"],\n",
    "    )\n",
    "    print(statistics_dict)\n",
    "    # np.save(ds.get_cache_dir() / \"statistics.npy\", statistics_dict)\n",
    "\n",
    "\n",
    "def normalize_sample(stats, surface, upper):\n",
    "    if len(stats[\"mean_surface\"].shape) == len(surface.shape):\n",
    "        norm_surface = (surface - stats[\"mean_surface\"]) / stats[\"std_surface\"]\n",
    "        norm_upper = (upper - stats[\"mean_upper\"]) / stats[\"std_upper\"]\n",
    "    else:\n",
    "        norm_surface = (surface - stats[\"mean_surface\"][..., None]) / stats[\n",
    "            \"std_surface\"\n",
    "        ][..., None]\n",
    "        norm_upper = (upper - stats[\"mean_upper\"][..., None]) / stats[\"std_upper\"][\n",
    "            ..., None\n",
    "        ]\n",
    "    return norm_surface, norm_upper\n",
    "\n",
    "\n",
    "def denormalize_sample(stats, surface, upper):\n",
    "    if len(stats[\"mean_surface\"].shape) == len(surface.shape[1:]):\n",
    "        denorm_surface = surface * stats[\"std_surface\"] + stats[\"mean_surface\"]\n",
    "        denorm_upper = upper * stats[\"std_upper\"] + stats[\"mean_upper\"]\n",
    "    else:\n",
    "        denorm_surface = (\n",
    "            surface * stats[\"std_surface\"][..., None] + stats[\"mean_surface\"][..., None]\n",
    "        )\n",
    "        denorm_upper = (\n",
    "            upper * stats[\"std_upper\"][..., None] + stats[\"mean_upper\"][..., None]\n",
    "        )\n",
    "    return denorm_surface, denorm_upper\n",
    "\n",
    "\n",
    "def _get_stats_at_idx(idx_and_nside_tuple):\n",
    "    idx, nside = idx_and_nside_tuple\n",
    "    ds = DataHP(DataHPConfig(nside=nside, cache=False, normalized=False))\n",
    "    sample = ds[idx]\n",
    "    mean_surface = (\n",
    "        sample[\"input_surface\"].astype(np.float64).mean(axis=1, keepdims=True)\n",
    "    )\n",
    "    mean_upper = sample[\"input_upper\"].astype(np.float64).mean(axis=2, keepdims=True)\n",
    "    mean_x2_surface = (sample[\"input_surface\"].astype(np.float64) ** 2).mean(\n",
    "        axis=1, keepdims=True\n",
    "    )\n",
    "    mean_x2_upper = (sample[\"input_upper\"].astype(np.float64) ** 2).mean(\n",
    "        axis=2, keepdims=True\n",
    "    )\n",
    "    return mean_surface, mean_upper, mean_x2_surface, mean_x2_upper\n",
    "\n",
    "\n",
    "def serialize_dataset_statistics(nside, test_with_one_sample=False):\n",
    "    ds = DataHP(DataHPConfig(nside=nside, cache=False, normalized=False))\n",
    "\n",
    "    if test_with_one_sample:\n",
    "        n_samples = 1\n",
    "        n_samples_left = 1\n",
    "    else:\n",
    "        n_samples = len(ds)\n",
    "        n_samples_left = len(ds)\n",
    "\n",
    "    sample = ds[0]\n",
    "    mean_surface = np.zeros_like(\n",
    "        sample[\"input_surface\"].mean(axis=1, keepdims=True), dtype=np.float64\n",
    "    )\n",
    "    mean_upper = np.zeros_like(\n",
    "        sample[\"input_upper\"].mean(axis=2, keepdims=True), dtype=np.float64\n",
    "    )\n",
    "    mean_x2_surface = np.zeros_like(mean_surface)\n",
    "    mean_x2_upper = np.zeros_like(mean_upper)\n",
    "\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    batch_size = int(os.getenv(\"SLURM_CPUS_ON_NODE\", f\"{os.cpu_count()}\"))\n",
    "    print(f\"Starting with batch size {batch_size}\")\n",
    "    idx = 0\n",
    "\n",
    "    # n_samples_left = 34\n",
    "\n",
    "    with Pool(batch_size) as p:\n",
    "        while n_samples_left > 0:\n",
    "            n_requests = min(batch_size, n_samples_left)\n",
    "            chunks = zip(range(idx, idx + n_requests), [nside] * n_requests)\n",
    "            start_time = time.time()\n",
    "            res = p.map(_get_stats_at_idx, chunks)\n",
    "            idx += n_requests\n",
    "            n_samples_left -= n_requests\n",
    "            for (\n",
    "                sample_mean_surface,\n",
    "                sample_mean_upper,\n",
    "                sample_mean_x2_surface,\n",
    "                sample_mean_x2_upper,\n",
    "            ) in res:\n",
    "                mean_surface += sample_mean_surface\n",
    "                mean_upper += sample_mean_upper\n",
    "                mean_x2_surface += sample_mean_x2_surface\n",
    "                mean_x2_upper += sample_mean_x2_upper\n",
    "\n",
    "            print(\n",
    "                f\"{n_requests} samples in {(time.time() - start_time):.02f}s, {n_samples_left} left\"\n",
    "            )\n",
    "\n",
    "    mean_surface = mean_surface / n_samples\n",
    "    mean_upper = mean_upper / n_samples\n",
    "    mean_x2_surface = mean_x2_surface / n_samples\n",
    "    mean_x2_upper = mean_x2_upper / n_samples\n",
    "\n",
    "    std_surface = np.sqrt(mean_x2_surface - (mean_surface) ** 2)\n",
    "    std_upper = np.sqrt(mean_x2_upper - (mean_upper) ** 2)\n",
    "    statistics_dict = dict(\n",
    "        mean_surface=mean_surface,\n",
    "        mean_upper=mean_upper,\n",
    "        mean_x2_surface=mean_x2_surface,\n",
    "        mean_x2_upper=mean_x2_upper,\n",
    "        std_surface=std_surface,\n",
    "        std_upper=std_upper,\n",
    "        n_samples=n_samples,\n",
    "    )\n",
    "    ds.get_cache_dir().mkdir(parents=True, exist_ok=True)\n",
    "    np.save(ds.get_statistics_path(), statistics_dict)\n",
    "    print(f\"Saved npy {ds.get_statistics_path()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493c9f2f-10fb-4502-8328-61a626bf2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from persisted config files\n",
    "def create_config(ensemble_id, epoch=200, dataset_years=10):\n",
    "    loss = torch.nn.L1Loss()\n",
    "\n",
    "    def reg_loss(output, batch):\n",
    "        # breakpoint()\n",
    "        # breakpoint()\n",
    "        # return loss(output[\"logits_upper\"], batch[\"target_upper\"])  # + 0.25 * loss(\n",
    "\n",
    "        return loss(output[\"logits_upper\"], batch[\"target_upper\"]) + 0.25 * loss(\n",
    "            output[\"logits_surface\"], batch[\"target_surface\"]\n",
    "        )\n",
    "        # return loss(output[\"logits_surface\"], batch[\"target_surface\"])\n",
    "\n",
    "    train_config = TrainConfig(\n",
    "        extra=dict(loss_variant=\"full\"),\n",
    "        model_config=SwinHPPanguPadConfig(\n",
    "            base_pix=12,\n",
    "            nside=NSIDE,\n",
    "            dev_mode=False,\n",
    "            depths=[2, 6, 6, 2],\n",
    "            # num_heads=[6, 12, 12, 6],\n",
    "            num_heads=[6, 12, 12, 6],\n",
    "            # embed_dims=[192, 384, 384, 192],\n",
    "            embed_dims=[192 // 4, 384 // 4, 384 // 4, 192 // 4],\n",
    "            # embed_dims=[16, 384 // 16, 384 // 16, 192 // 16],\n",
    "            # embed_dims=[x for x in [16, 32, 32, 16]],\n",
    "            window_size=[2, 64],  # int(32 * (NSIDE / 256)),\n",
    "            use_cos_attn=False,\n",
    "            use_v2_norm_placement=True,\n",
    "            drop_rate=0,  # ,0.1,\n",
    "            attn_drop_rate=0,  # ,0.1,\n",
    "            drop_path_rate=0,\n",
    "            rel_pos_bias=\"single\",\n",
    "            # shift_size=8,  # int(16 * (NSIDE / 256)),\n",
    "            shift_size=4,  # int(16 * (NSIDE / 256)),\n",
    "            shift_strategy=\"ring_shift\",\n",
    "            ape=False,\n",
    "            patch_size=16,\n",
    "        ),\n",
    "        train_data_config=DataHPConfig(nside=NSIDE, end_year=2007 + dataset_years),\n",
    "        val_data_config=None,  # DataHPConfig(nside=NSIDE),\n",
    "        loss=reg_loss,\n",
    "        optimizer=OptimizerConfig(\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            kwargs=dict(weight_decay=3e-6, lr=5e-4),\n",
    "            # kwargs=dict(weight_decay=3e-6, lr=5e-3),\n",
    "        ),\n",
    "        batch_size=1,\n",
    "        ensemble_id=ensemble_id,\n",
    "        # gradient_clipping=0.3,\n",
    "        # _version=57,\n",
    "        # NOTE: Versions below 13 used buggy window shifting\n",
    "        _version=17,\n",
    "        # _version=55,\n",
    "    )\n",
    "    train_eval = TrainEval(\n",
    "        train_metrics=[create_metric(reg_loss)],\n",
    "        validation_metrics=[],\n",
    "        log_gradient_norm=True,\n",
    "    )  # create_regression_metrics(torch.nn.functional.l1_loss, None)\n",
    "    train_run = TrainRun(\n",
    "        project=\"weather\",\n",
    "        # compute_config=ComputeConfig(distributed=False, num_workers=0, num_gpus=1),\n",
    "        # compute_config=ComputeConfig(distributed=False, num_workers=5, num_gpus=1),\n",
    "        # compute_config=ComputeConfig(distributed=True, num_workers=5, num_gpus=4),\n",
    "        compute_config=ComputeConfig(),\n",
    "        train_config=train_config,\n",
    "        train_eval=train_eval,\n",
    "        epochs=epoch,\n",
    "        save_nth_epoch=1,\n",
    "        keep_epoch_checkpoints=True,\n",
    "        keep_nth_epoch_checkpoints=10,\n",
    "        validate_nth_epoch=20,\n",
    "        visualize_terminal=False,\n",
    "        notes=dict(shift=\"fixed: ring shift uses shift_size instead of window/2\"),\n",
    "    )\n",
    "    return train_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1c882-c224-422e-8cef-5d09cae98582",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_config = create_ensemble_config(\n",
    "    lambda eid: create_config(eid, dataset_years=dataset_years), 1\n",
    ")\n",
    "request_ensemble(ensemble_config)\n",
    "distributed_train(ensemble_config.members)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
